[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hello World!",
    "section": "",
    "text": "I’m Thanh, and I’m a current MS: Business Analytics student at CUNY Baruch. Nice to meet you!\nFeel free to check out my resume, which includes my contact information (i.e. email, cell phone number).\nAdd me on LinkedIn!"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project #01",
    "section": "",
    "text": "Author: Thanh Dao\nUpdated: October 3rd, 2024"
  },
  {
    "objectID": "mp01.html#fare-revenue",
    "href": "mp01.html#fare-revenue",
    "title": "mp01",
    "section": "Fare Revenue",
    "text": "Fare Revenue\nlibrary(tidyverse) if(!file.exists(“2022_fare_revenue.xlsx”)){ download.file(“http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx”, destfile=“2022_fare_revenue.xlsx”, quiet=FALSE, method=“wget”) } FARES &lt;- readxl::read_xlsx(“2022_fare_revenue.xlsx”) |&gt; select(-State/Parent NTD ID, -Reporter Type, -Reporting Module, -TOS, -Passenger Paid Fares, -Organization Paid Fares) |&gt; filter(Expense Type == “Funds Earned During Period”) |&gt; select(-Expense Type)"
  },
  {
    "objectID": "mp01.html#expenses",
    "href": "mp01.html#expenses",
    "title": "mp01",
    "section": "Expenses",
    "text": "Expenses\nif(!file.exists(“2022_expenses.csv”)){ download.file(“https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true”, destfile=“2022_expenses.csv”, quiet=FALSE, method=“wget”) } EXPENSES &lt;- readr::read_csv(“2022_expenses.csv”) |&gt; select(NTD ID, Agency, Total, Mode) |&gt; mutate(NTD ID = as.integer(NTD ID)) |&gt; rename(Expenses = Total) |&gt; group_by(NTD ID, Mode) |&gt; summarize(Expenses = sum(Expenses)) |&gt; ungroup()\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(NTD ID, Mode))"
  },
  {
    "objectID": "mp01.html#monthly-transit-numbers",
    "href": "mp01.html#monthly-transit-numbers",
    "title": "mp01",
    "section": "Monthly Transit Numbers",
    "text": "Monthly Transit Numbers\nlibrary(tidyverse) if(!file.exists(“ridership.xlsx”)){ download.file(“https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx”, destfile=“ridership.xlsx”, quiet=FALSE, method=“wget”) } library(lubridate) # added for the my() function\nTRIPS &lt;- readxl::read_xlsx(“ridership.xlsx”, sheet=“UPT”) |&gt; filter(Mode/Type of Service Status == “Active”) |&gt; select(-Legacy NTD ID, -Reporter Type, -Mode/Type of Service Status, -UACE CD, -TOS) |&gt; pivot_longer(-c(NTD ID:3 Mode), names_to=“month”, values_to=“UPT”) |&gt; drop_na() |&gt; mutate(month=my(month)) MILES &lt;- readxl::read_xlsx(“ridership.xlsx”, sheet=“VRM”) |&gt; filter(Mode/Type of Service Status == “Active”) |&gt; select(-Legacy NTD ID, -Reporter Type, -Mode/Type of Service Status, -UACE CD, -TOS) |&gt; pivot_longer(-c(NTD ID:3 Mode), names_to=“month”, values_to=“VRM”) |&gt; drop_na() |&gt; group_by(NTD ID, Agency, UZA Name, Mode, 3 Mode, month) |&gt; summarize(VRM = sum(VRM)) |&gt; ungroup() |&gt; mutate(month=my(month))\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt; mutate(NTD ID = as.integer(NTD ID))"
  },
  {
    "objectID": "mp01.html#task-1-create-syntactic-names",
    "href": "mp01.html#task-1-create-syntactic-names",
    "title": "mp01",
    "section": "Task 1: Create Syntactic Names",
    "text": "Task 1: Create Syntactic Names\nUSAGE &lt;- rename(USAGE, “metro_area” = “UZA Name”) USAGE &lt;- rename(USAGE, “unlinked_passenger_trips” = “UPT”) USAGE &lt;- rename(USAGE, “vehicle_revenue_miles” = “VRM”)"
  },
  {
    "objectID": "mp01.html#task-2-recoding-the-mode-column",
    "href": "mp01.html#task-2-recoding-the-mode-column",
    "title": "mp01",
    "section": "Task 2: Recoding the Mode Column",
    "text": "Task 2: Recoding the Mode Column\nunique(USAGE$Mode) # Find the unique Mode codes\nUSAGE &lt;- USAGE |&gt; # Interpret the Mode column mutate(Mode = case_when( Mode == “AR” ~ “Alaska Railroad”, Mode == “CB” ~ “Commuter Bus”, Mode == “CC” ~ “Cable Car”, Mode == “CR” ~ “Commuter Rail”, Mode == “DR” ~ “Demand Response”, Mode == “FB” ~ “Ferryboat”, Mode == “HR” ~ “Heavy Rail”, Mode == “IP” ~ “Inclined Plane”, Mode == “LR” ~ “Light Rail”, Mode == “MB” ~ “Bus”, Mode == “MG” ~ “Monorail and Automated Guideway”, Mode == “PB” ~ “Publico”, Mode == “RB” ~ “Bus Rapid Transit”, Mode == “SR” ~ “Streetcar Rail”, Mode == “TB” ~ “Trolleybus”, Mode == “TR” ~ “Aerial Tramways”, Mode == “VP” ~ “Vanpool”, Mode == “YR” ~ “Hybrid Rail”, TRUE ~ “Unknown”))\nUSAGE &lt;- USAGE |&gt; # Remove “NTD ID” and “3 Mode” select(-c(“NTD ID”, “3 Mode”))\nif(!require(“DT”)) install.packages(“DT”) library(DT)\nsample_n(USAGE, 1000) |&gt; mutate(month=as.character(month)) |&gt; DT::datatable()"
  },
  {
    "objectID": "mp01.html#task-3-answering-instructor-specified-questions-with-dplyr",
    "href": "mp01.html#task-3-answering-instructor-specified-questions-with-dplyr",
    "title": "mp01",
    "section": "Task 3: Answering Instructor Specified Questions with dplyr",
    "text": "Task 3: Answering Instructor Specified Questions with dplyr"
  },
  {
    "objectID": "mp01.html#task-4-explore-and-analyze",
    "href": "mp01.html#task-4-explore-and-analyze",
    "title": "mp01",
    "section": "Task 4: Explore and Analyze",
    "text": "Task 4: Explore and Analyze"
  },
  {
    "objectID": "mp01.html#task-5-table-summarization",
    "href": "mp01.html#task-5-table-summarization",
    "title": "mp01",
    "section": "Task 5: Table Summarization",
    "text": "Task 5: Table Summarization"
  },
  {
    "objectID": "mp01.html#task-6-farebox-recovery-among-major-systems",
    "href": "mp01.html#task-6-farebox-recovery-among-major-systems",
    "title": "mp01",
    "section": "Task 6: Farebox Recovery Among Major Systems",
    "text": "Task 6: Farebox Recovery Among Major Systems"
  },
  {
    "objectID": "Lab Activity 1: Part 1.html",
    "href": "Lab Activity 1: Part 1.html",
    "title": "Lab Activity 1: Part 1",
    "section": "",
    "text": "If you’d like to reach me, you can contact me in the following ways:\n\nAdd me on [LinkedIn](https://www.linkedin.com/in/thanht-dao/)!\nEmail me at daothanht922@gmail.com"
  },
  {
    "objectID": "mp01.html#introduction",
    "href": "mp01.html#introduction",
    "title": "Mini-Project #01",
    "section": "Introduction",
    "text": "Introduction\nThere is a great variety of transportation systems and modes in America, spanning across land and water. Americans utilize all of them on a daily basis, commuting to wherever they need to go. In this report, the fiscal characteristics of major US public transit systems will be analyzed."
  },
  {
    "objectID": "mp01.html#findings",
    "href": "mp01.html#findings",
    "title": "Mini-Project #01",
    "section": "Findings",
    "text": "Findings"
  },
  {
    "objectID": "mp01.html#analysis",
    "href": "mp01.html#analysis",
    "title": "Mini-Project #01",
    "section": "Analysis",
    "text": "Analysis\nIn this sample, we can determine a few things.\nIf we were to examine the most total vehicle miles through the lens of transit mode, buses had the highest, with 49,444,494,088 miles.\nNow let’s take a closer look at the MTA New York City Transit, which was the agency with the most total vehicle revenue miles, of approximately 10,800,000,000 miles. When examining the MTA heavy rail, it can be determined that 180,458,819 trips were taken in May 2024.\nIn contrast to these high numbers, due to the COVID-19 pandemic, we can see the fall of NYC subway ridership fall between the time of April 2019 and April 2020, with ridership falling from 232,223,929 to 20,254,269, respectively. This points to a difference of 211,969,660 rides."
  },
  {
    "objectID": "mp01.html#farebox-recovery-analysis",
    "href": "mp01.html#farebox-recovery-analysis",
    "title": "Mini-Project #01",
    "section": "Farebox Recovery Analysis",
    "text": "Farebox Recovery Analysis\nTo fully analyze the farebox recovery data, a new table must be created from the previous table, narrowing down the data to specifically 2022. First, the obtained data needs to be edited to create the summary table.\n\n## Task 5: Table Summarization\nUSAGE_2022_ANNUAL &lt;- USAGE |&gt;\n  mutate(year = year(month)) |&gt;\n  filter(year == 2022) |&gt;\n  group_by(NTD_ID, \n           Agency, \n           metro_area, \n           Mode, \n           unlinked_passenger_trips, \n           vehicle_revenue_miles) |&gt;\n  summarize(\n    total_upt = sum(unlinked_passenger_trips, na.rm = T),\n    total_vrm = sum(vehicle_revenue_miles, na.rm = T),\n    .groups = \"keep\",\n  ) |&gt;\n  ungroup()\n\nUSAGE_AND_FINANCIALS &lt;- left_join(USAGE_2022_ANNUAL, \n                                  FINANCIALS, \n                                  join_by(NTD_ID, Mode),\n                                  relationship = \"many-to-many\") |&gt;\n  drop_na()\n\nSecond, the table can be created.\n\n\n\n\n\n\n\nWhen analyzing the farebox recovery, it can be determined that the transit system with the most UPT in 2022 was the MTA New York CIty Transit, specifically the heavy rail, with 1,793,073,801 trips.\n\nmostUPT2022 &lt;- USAGE_AND_FINANCIALS |&gt;   # Create a new variable\n  group_by(Agency, Mode) |&gt;   # Look through the lens of Agency and Mode\n  filter(total_upt &gt; 400000) |&gt;  # Major Transit systems\n  summarize(total_upt2022 = sum(total_upt)) |&gt;  # Obtain the total UPT per what was grouped\n  arrange(desc(total_upt2022))  # Descending order\nhead(mostUPT2022, n=1)   # Get only the wanted data\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                    Mode       total_upt2022\n  &lt;chr&gt;                     &lt;chr&gt;              &lt;dbl&gt;\n1 MTA New York City Transit Heavy Rail    1793073801\n\n\nHowever, if examining the transit system with the highest farebox recovery, it would be the Anaheim Transportation Network, specifically the bus, with 0.865.\n\nhighestfarebox &lt;- USAGE_AND_FINANCIALS |&gt;   # Create a new variable \n  group_by(Agency, Mode) |&gt;   # Look through the lens of Agency and Mode\n  filter(total_upt &gt; 400000) |&gt;  # Major Transit systems\n  summarize(highestfarebox = sum(`Total Fares`) / sum (Expenses)) |&gt;  # Obtain the ratio of total fares to expenses\n  arrange(desc(highestfarebox))  # Descending order\nhead(highestfarebox, n=1)   # Get only the wanted data\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                         Mode  highestfarebox\n  &lt;chr&gt;                          &lt;chr&gt;          &lt;dbl&gt;\n1 Anaheim Transportation Network Bus            0.865\n\n\nMoving on, when examining the transit system with the lowest expenses per UPT, it would be the University of Georgia bus system, with $14.90 per trip.\n\nlowestexpenses &lt;- USAGE_AND_FINANCIALS |&gt;   # Create a new variable\n  group_by(Agency, Mode) |&gt;   # Look through the lens of Agency and Mode\n  filter(total_upt &gt; 400000) |&gt;  # Major Transit systems\n  summarize(lowestexpenses = sum(Expenses) / sum(total_upt)) |&gt;   # Obtain the ratio of expenses to UPT\n  arrange(desc(lowestexpenses))   # Descending order\ntail(lowestexpenses, n=1)   # Get only the wanted data\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                Mode  lowestexpenses\n  &lt;chr&gt;                 &lt;chr&gt;          &lt;dbl&gt;\n1 University of Georgia Bus             14.9\n\n\nComparatively, the transit system with lowest expenses per VRM is the Interurban Transit Partnership bus system, with $84.10 per mile.\n\nlowestexpensesvrm &lt;- USAGE_AND_FINANCIALS |&gt;   # Create a new variable \n  group_by(Agency, Mode) |&gt;   # Look through the lens of Agency and Mode\n  filter(total_upt &gt; 400000) |&gt;  # Major Transit systems\n  summarize(lowestexpensesvrm = sum(Expenses) / sum(total_vrm)) |&gt;   # Obtain the ratio of expenses to VRM\n  arrange(desc(lowestexpensesvrm))   # Descending order\ntail(lowestexpensesvrm, n=1)   # Get only the wanted data\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                         Mode  lowestexpensesvrm\n  &lt;chr&gt;                          &lt;chr&gt;             &lt;dbl&gt;\n1 Interurban Transit Partnership Bus                83.1\n\n\nMeanwhile, the transit system with the highest total fares per UPT is the Metro-North Commuter Railroad Company, with the bus, at $98.70 per trip.\n\nhighestfares &lt;- USAGE_AND_FINANCIALS |&gt;   # Create a new variable \n  group_by(Agency, Mode) |&gt;   # Look through the lens of Agency and Mode\n  filter(total_upt &gt; 400000) |&gt;  # Major Transit systems\n  summarize(highestfares = sum(`Total Fares`) / sum(total_upt)) |&gt;   # Obtain the ratio of total fares to UPT\n  arrange(desc(highestfares))   # Descending order\nhead(highestfares, n=1)   # Get only the wanted data\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                                                      Mode  highestfares\n  &lt;chr&gt;                                                       &lt;chr&gt;        &lt;dbl&gt;\n1 Metro-North Commuter Railroad Company, dba: MTA Metro-Nort… Comm…         98.7\n\n\nComparatively, the transit system with highest total fares per VRM is the Washington State Ferries with the ferryboat, at $937 per mile.\n\nhighestfaresvrm &lt;- USAGE_AND_FINANCIALS |&gt;   # Create a new variable\n  group_by(Agency, Mode) |&gt;   # Look through the lens of Agency and Mode\n  filter(total_upt &gt; 400000) |&gt;   # Major Transit systems\n  summarize(highestfaresvrm = sum(`Total Fares`) / sum(total_vrm)) |&gt;  # obtain the ratio of total fares to VRM\n  arrange(desc(highestfaresvrm))   # Descending order\nhead(highestfaresvrm, n=1)   # Get only the wanted data\n\n# A tibble: 1 × 3\n# Groups:   Agency [1]\n  Agency                   Mode      highestfaresvrm\n  &lt;chr&gt;                    &lt;chr&gt;               &lt;dbl&gt;\n1 Washington State Ferries Ferryboat            937."
  },
  {
    "objectID": "mp01.html#conclusions",
    "href": "mp01.html#conclusions",
    "title": "Mini-Project #01",
    "section": "Conclusions",
    "text": "Conclusions\nOverall, the most efficient transit system in the country is the MTA New York City Transit. It is the largest North American transit system, and has proven that it is as efficient as its size. It has the most total and average vehicle revenue miles. Additionally, in 2022, it was the transit system with the most unlinked passenger trips."
  },
  {
    "objectID": "mp01.html#main-analysis",
    "href": "mp01.html#main-analysis",
    "title": "Mini-Project #01",
    "section": "Main Analysis",
    "text": "Main Analysis\nUsing this table, the following analyses can be done.\nThe transit agency with the most total VRM in this data set can be determined with the following code:\n\nUSAGE |&gt;   \n  group_by(Agency) |&gt;\n  summarize(total_vrm = sum(vehicle_revenue_miles, na.rm = T)) |&gt;\n  arrange(desc(total_vrm)) |&gt;\n  slice(1)\n\n# A tibble: 1 × 2\n  Agency                      total_vrm\n  &lt;chr&gt;                           &lt;dbl&gt;\n1 MTA New York City Transit 10832855350\n\n\nBased on this, it can be determined that the MTA New York City Transit had the most total VRM, of approximately 10,800,000,000 miles.\nComparatively, a similar code can be used to determine the transit mode with the most total VRM:\n\nUSAGE |&gt;\n  group_by(Mode) |&gt;\n  summarize(total_vrm = sum(vehicle_revenue_miles, na.rm = T)) |&gt;\n  arrange(desc(total_vrm)) |&gt;\n  slice(1)\n\n# A tibble: 1 × 2\n  Mode    total_vrm\n  &lt;chr&gt;       &lt;dbl&gt;\n1 Bus   49444494088\n\n\nIt can be determined that buses had the most total VRM, with 49,444,494,088 miles.\nNow, a closer look will be had on the MTA, specifically the number of trips taken on the NYC Subway in May 2024:\n\nUSAGE$month &lt;- as.character(USAGE$month)  \nUSAGE |&gt;\n  filter(USAGE$Agency == \"MTA New York City Transit\",\n         USAGE$Mode == \"Heavy Rail\",\n         USAGE$month == \"2024-05-01\")\n\n# A tibble: 1 × 7\n  NTD_ID Agency                    metro_area Mode  month unlinked_passenger_t…¹\n   &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;\n1  20008 MTA New York City Transit New York-… Heav… 2024…              180458819\n# ℹ abbreviated name: ¹​unlinked_passenger_trips\n# ℹ 1 more variable: vehicle_revenue_miles &lt;dbl&gt;\n\n\nHere, in May 2024, 180,458,819 trips were taken.\nWhile this number is indeed high, it would be interesting to analyze the impact the COVID-19 pandemic had on NYC Subway ridership. To do so, the fall of NYC subway ridership between April 2019 and April 2020 needs to be determined:\n\nUSAGE |&gt; \n  filter(USAGE$Agency == \"MTA New York City Transit\", #2019\n         USAGE$Mode == \"Heavy Rail\",\n         USAGE$month == \"2019-04-01\")\n\n# A tibble: 1 × 7\n  NTD_ID Agency                    metro_area Mode  month unlinked_passenger_t…¹\n   &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;\n1  20008 MTA New York City Transit New York-… Heav… 2019…              232223929\n# ℹ abbreviated name: ¹​unlinked_passenger_trips\n# ℹ 1 more variable: vehicle_revenue_miles &lt;dbl&gt;\n\nUSAGE |&gt;\n  filter(USAGE$Agency == \"MTA New York City Transit\", #2020\n         USAGE$Mode == \"Heavy Rail\",\n         USAGE$month == \"2020-04-01\")\n\n# A tibble: 1 × 7\n  NTD_ID Agency                    metro_area Mode  month unlinked_passenger_t…¹\n   &lt;int&gt; &lt;chr&gt;                     &lt;chr&gt;      &lt;chr&gt; &lt;chr&gt;                  &lt;dbl&gt;\n1  20008 MTA New York City Transit New York-… Heav… 2020…               20254269\n# ℹ abbreviated name: ¹​unlinked_passenger_trips\n# ℹ 1 more variable: vehicle_revenue_miles &lt;dbl&gt;\n\n\nBased on this information, ridership fell from 232,223,929 in April 2019 to 20,254,269 in April 2020. This points to a difference of 211,969,660 rides."
  },
  {
    "objectID": "mp01.html#additional-analysis",
    "href": "mp01.html#additional-analysis",
    "title": "Mini-Project #01",
    "section": "Additional Analysis",
    "text": "Additional Analysis\nPreviously, buses were determined to be the transit mode with the most total VRM. It would be interesting to determine the opposite:\n\nUSAGE |&gt; \n  group_by(Mode) |&gt;   # Look through the lens of transportation mode\n  summarize(total_vrm = sum(vehicle_revenue_miles, na.rm = T)) |&gt;   # Obtain the total VRM per mode\n  arrange(total_vrm) |&gt;   # Ascending order\n  slice(1)   # Obtain the top data point\n\n# A tibble: 1 × 2\n  Mode            total_vrm\n  &lt;chr&gt;               &lt;dbl&gt;\n1 Aerial Tramways    292860\n\n\nIn contrast to buses, aerial tramways were the transit mode with the least total VRM, with 292,850 miles.\nThis is a difference of 49,444,201,238 miles. While this might feel drastic, it makes sense. Aerial tramways, vehicles suspended from a system of cables that are propelled through a suspension system, are not commonly used, which attributes to their low mileage. On the other hand, buses are used extensively, with there being numerous routes that run daily, attributing to their high mileage.\nFollowing this contrast, the agency with the least total VRM was Barnegat Bay Decoy & Baymen’s Museum, with 2,312 miles.\n\nUSAGE |&gt;\n  group_by(Agency) |&gt;   # Look through the lens of transportation agency   \n  summarize(total_vrm = sum(vehicle_revenue_miles, na.rm = T)) |&gt;   # Obtain the total VRM per agency\n  arrange(total_vrm) |&gt;   # Ascending order\n  slice(1)   # Obtain the top data point\n\n# A tibble: 1 × 2\n  Agency                               total_vrm\n  &lt;chr&gt;                                    &lt;dbl&gt;\n1 Barnegat Bay Decoy & Baymen's Museum      2312\n\n\nCompared to the MTA, there is a difference of 10,799,997,688 miles. This large difference can be attributed to the vast difference in sizes between the two. The MTA is the largest transportation system in North America. On the other hand, Barnegat is only a 40 acre cultural center meant to preserve the maritime history of the Jersey Shore. The VRM accumulated correlate to the size and population attributed to each agency.\nBecause the MTA in NYC has been established as having the largest VRM, examining which location has the second highest average vehicle revenue miles, after the New York / New Jersey area, would be interesting.\n\nUSAGE |&gt; \n  group_by(metro_area) |&gt;   # Look through the lens of location\n  summarize(average_vrm = mean(vehicle_revenue_miles, na.rm = T)) |&gt; # Obtain the average VRM per location\n  arrange(desc(average_vrm)) |&gt;   # Descending order\n  slice(2)   # Obtain the second data point\n\n# A tibble: 1 × 2\n  metro_area         average_vrm\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Denver--Aurora, CO    1565688.\n\n\nThis turns out to be Denver, Colorado, with 1,565,688 average VRM. This is a 244,360 mile difference to NYC, with 1,810,058 miles."
  },
  {
    "objectID": "mp01.html#obtaining-data",
    "href": "mp01.html#obtaining-data",
    "title": "Mini-Project #01",
    "section": "Obtaining Data",
    "text": "Obtaining Data\nThis report runs on R; thus, the necessary packages must be installed prior to any data collection or analysis.\n\n# Install the necessary R packages\nif(!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(lubridate)  # added for the my() function\nif(!require(\"DT\")) install.packages(\"DT\")\nlibrary(DT)\n\nNow that the necessary packages have been installed, data collection from the National Transit Database can proceed.\n\n# Download the \"Fare Revenue\" Data\nif(!file.exists(\"2022_fare_revenue.xlsx\")){\n  download.file(\"http://www.transit.dot.gov/sites/fta.dot.gov/files/2024-04/2022%20Fare%20Revenue.xlsx\", \n                destfile=\"2022_fare_revenue.xlsx\", \n                quiet=FALSE, \n                method=\"wget\")\n}\n\n# Select the columns within the scope of research for this report\nFARES &lt;- readxl::read_xlsx(\"2022_fare_revenue.xlsx\") |&gt;\n  select(-`State/Parent NTD ID`, \n         -`Reporter Type`,\n         -`Reporting Module`,\n         -`TOS`,\n         -`Passenger Paid Fares`,\n         -`Organization Paid Fares`) |&gt;\n  filter(`Expense Type` == \"Funds Earned During Period\") |&gt;\n  select(-`Expense Type`)\n\n# Download the \"Expenses\" Data\nif(!file.exists(\"2022_expenses.csv\")){\n  download.file(\"https://data.transportation.gov/api/views/dkxx-zjd6/rows.csv?date=20231102&accessType=DOWNLOAD&bom=true&format=true\", \n                destfile=\"2022_expenses.csv\", \n                quiet=FALSE, \n                method=\"wget\")\n}\n\n# Select the columns within the scope of research for this report\nEXPENSES &lt;- readr::read_csv(\"2022_expenses.csv\") |&gt;\n  select(`NTD ID`, \n         `Agency`,\n         `Total`, \n         `Mode`) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`)) |&gt;\n  rename(Expenses = Total) |&gt;\n  group_by(`NTD ID`, `Mode`) |&gt;\n  summarize(Expenses = sum(Expenses)) |&gt;\n  ungroup()\n\n# Combine the selected \"Fare Revenue\" and \"Expenses\" Data\nFINANCIALS &lt;- inner_join(FARES, EXPENSES, join_by(`NTD ID`, `Mode`)) \n\n# Download the \"Monthly Transit Numbers\" Data\nif(!file.exists(\"ridership.xlsx\")){\n  download.file(\"https://www.transit.dot.gov/sites/fta.dot.gov/files/2024-09/July%202024%20Complete%20Monthly%20Ridership%20%28with%20adjustments%20and%20estimates%29_240903.xlsx\", \n                destfile=\"ridership.xlsx\", \n                quiet=FALSE, \n                method=\"wget\")\n}\n\n# Select the columns within the scope of research for this report\nTRIPS &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"UPT\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"UPT\") |&gt;\n  drop_na() |&gt;\n  mutate(month=my(month)) \nMILES &lt;- readxl::read_xlsx(\"ridership.xlsx\", sheet=\"VRM\") |&gt;\n  filter(`Mode/Type of Service Status` == \"Active\") |&gt;\n  select(-`Legacy NTD ID`, \n         -`Reporter Type`, \n         -`Mode/Type of Service Status`, \n         -`UACE CD`, \n         -`TOS`) |&gt;\n  pivot_longer(-c(`NTD ID`:`3 Mode`), \n               names_to=\"month\", \n               values_to=\"VRM\") |&gt;\n  drop_na() |&gt;\n  group_by(`NTD ID`, `Agency`, `UZA Name`, \n           `Mode`, `3 Mode`, month) |&gt;\n  summarize(VRM = sum(VRM)) |&gt;\n  ungroup() |&gt;\n  mutate(month=my(month))\n\n# Combine the selected \"Monthly Transit Numbers\" Data\nUSAGE &lt;- inner_join(TRIPS, MILES) |&gt;\n  mutate(`NTD ID` = as.integer(`NTD ID`))"
  },
  {
    "objectID": "mp01.html#initial-data-table",
    "href": "mp01.html#initial-data-table",
    "title": "Mini-Project #01",
    "section": "Initial Data Table",
    "text": "Initial Data Table\nUsing the data just obtained from the National Transit Database, the following table can be created:\n\n\n\n\n\n\n\nUnfortunately, this data still some flaws that need to be fixed."
  },
  {
    "objectID": "mp01.html#cleaning-the-data",
    "href": "mp01.html#cleaning-the-data",
    "title": "Mini-Project #01",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\nTo the average person, “UZA Name” doesn’t provide proper context or meaning; thus, it needs to be changed to something people will understand, like “Metro Area.” The same can be said about “UPT” and “VRM”; these also need to be changed, as follows:\n\n# Task 1: Create Syntactic Names\nUSAGE &lt;- rename(USAGE, \"metro_area\" = \"UZA Name\",\n                \"unlinked_passenger_trips\" = \"UPT\",\n                \"vehicle_revenue_miles\" = \"VRM\",\n                \"NTD_ID\" = \"NTD ID\")\nFINANCIALS &lt;- rename(FINANCIALS, \"NTD_ID\" = \"NTD ID\")\n\nNow that some column headers have been renamed, focus can be made on the “Mode” column. All of these modes of transport are acronyms that would need to be looked up to be understood. For user ease, they’ll be changed as follows:\n\n# Task 2: Recoding the Mode Column\nUSAGE &lt;- USAGE |&gt;                    # Interpret the Mode column in \"USAGE\"\n  mutate(Mode = case_when(\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    TRUE ~ \"Unknown\"))\n\nFINANCIALS &lt;- FINANCIALS |&gt;          # Interpret the Mode column in \"Financials\"\n  mutate(Mode = case_when(\n    Mode == \"AR\" ~ \"Alaska Railroad\",\n    Mode == \"CB\" ~ \"Commuter Bus\",\n    Mode == \"CC\" ~ \"Cable Car\",\n    Mode == \"CR\" ~ \"Commuter Rail\",\n    Mode == \"DR\" ~ \"Demand Response\",\n    Mode == \"FB\" ~ \"Ferryboat\",\n    Mode == \"HR\" ~ \"Heavy Rail\",\n    Mode == \"IP\" ~ \"Inclined Plane\",\n    Mode == \"LR\" ~ \"Light Rail\",\n    Mode == \"MB\" ~ \"Bus\",\n    Mode == \"MG\" ~ \"Monorail and Automated Guideway\",\n    Mode == \"PB\" ~ \"Publico\",\n    Mode == \"RB\" ~ \"Bus Rapid Transit\",\n    Mode == \"SR\" ~ \"Streetcar Rail\",\n    Mode == \"TB\" ~ \"Trolleybus\",\n    Mode == \"TR\" ~ \"Aerial Tramways\",\n    Mode == \"VP\" ~ \"Vanpool\",\n    Mode == \"YR\" ~ \"Hybrid Rail\",\n    TRUE ~ \"Unknown\"))\n\nUnnecessary columns will be removed as well.\n\nUSAGE &lt;- USAGE |&gt;           # Remove \"3 Mode\"\n  select(-c(\"3 Mode\"))\n\nNow that the data is cleaned up, a new table can be established, as follows:"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #02",
    "section": "",
    "text": "Author: Thanh Dao\nUpdated: October 22nd, 2024 @ 1:25PM"
  },
  {
    "objectID": "mp02.html#name_basics",
    "href": "mp02.html#name_basics",
    "title": "Mini-Project #02",
    "section": "NAME_BASICS",
    "text": "NAME_BASICS\n\n# Use the glimpse function\nglimpse(NAME_BASICS)\n\nRows: 2,460,608\nColumns: 6\n$ nconst            &lt;chr&gt; \"nm0000001\", \"nm0000002\", \"nm0000003\", \"nm0000004\", …\n$ primaryName       &lt;chr&gt; \"Fred Astaire\", \"Lauren Bacall\", \"Brigitte Bardot\", …\n$ birthYear         &lt;chr&gt; \"1899\", \"1924\", \"1934\", \"1949\", \"1918\", \"1915\", \"189…\n$ deathYear         &lt;chr&gt; \"1987\", \"2014\", \"\\\\N\", \"1982\", \"2007\", \"1982\", \"1957…\n$ primaryProfession &lt;chr&gt; \"actor,miscellaneous,producer\", \"actress,soundtrack,…\n$ knownForTitles    &lt;chr&gt; \"tt0072308,tt0050419,tt0053137,tt0027125\", \"tt003738…\n\n\nIn this table, we can see the non-standard “null” values are within the birthYear and deathYear columns. To fix this, the following code will be run:\n\n# Mutate the specified columns\nNAME_BASICS &lt;- suppressWarnings(\n  NAME_BASICS |&gt;\n  mutate(birthYear = as.numeric(birthYear),\n         deathYear = as.numeric(deathYear))\n  )"
  },
  {
    "objectID": "mp02.html#title_basics",
    "href": "mp02.html#title_basics",
    "title": "Mini-Project #02",
    "section": "TITLE_BASICS",
    "text": "TITLE_BASICS\n\n# Use the glimpse function\nglimpse(TITLE_BASICS)\n\nRows: 372,198\nColumns: 9\n$ tconst         &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt…\n$ titleType      &lt;chr&gt; \"short\", \"short\", \"short\", \"short\", \"short\", \"short\", \"…\n$ primaryTitle   &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ originalTitle  &lt;chr&gt; \"Carmencita\", \"Le clown et ses chiens\", \"Pauvre Pierrot…\n$ isAdult        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ startYear      &lt;dbl&gt; 1894, 1892, 1892, 1892, 1893, 1894, 1894, 1894, 1894, 1…\n$ endYear        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\",…\n$ runtimeMinutes &lt;chr&gt; \"1\", \"5\", \"5\", \"12\", \"1\", \"1\", \"1\", \"1\", \"45\", \"1\", \"1\"…\n$ genres         &lt;chr&gt; \"Documentary,Short\", \"Animation,Short\", \"Animation,Come…\n\n\nIn this table, we can see the non-standard “null” values are within the endYear, runtimeMinutes, and genres columns. To fix this, the following code will be run:\n\n# Mutate the specified columns\nTITLE_BASICS &lt;- suppressWarnings(\n  TITLE_BASICS |&gt;\n    mutate(runtimeMinutes = as.numeric(runtimeMinutes),\n           endYear = as.numeric(endYear),\n           genres = na_if(genres, \"\\\\N\"))\n)"
  },
  {
    "objectID": "mp02.html#title_crew",
    "href": "mp02.html#title_crew",
    "title": "Mini-Project #02",
    "section": "TITLE_CREW",
    "text": "TITLE_CREW\n\n# Use the glimpse function\nglimpse(TITLE_CREW)\n\nRows: 371,902\nColumns: 3\n$ tconst    &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt00000…\n$ directors &lt;chr&gt; \"nm0005690\", \"nm0721526\", \"nm0721526\", \"nm0721526\", \"nm00056…\n$ writers   &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"nm0…\n\n\nIn this table, we can see the non-standard “null” values are within the directors and writers columns. To fix this, the following code will be run:\n\n# Mutate the specified columns\nTITLE_CREW &lt;- suppressWarnings(\n  TITLE_CREW |&gt;\n    mutate(directors = na_if(directors, \"\\\\N\"),\n           writers = na_if(writers, \"\\\\N\"))\n)"
  },
  {
    "objectID": "mp02.html#title_episodes",
    "href": "mp02.html#title_episodes",
    "title": "Mini-Project #02",
    "section": "TITLE_EPISODES",
    "text": "TITLE_EPISODES\n\n# Use the glimpse function\nglimpse(TITLE_EPISODES)\n\nRows: 3,007,178\nColumns: 4\n$ tconst        &lt;chr&gt; \"tt0045960\", \"tt0046855\", \"tt0048378\", \"tt0048562\", \"tt0…\n$ parentTconst  &lt;chr&gt; \"tt0044284\", \"tt0046643\", \"tt0047702\", \"tt0047768\", \"tt0…\n$ seasonNumber  &lt;chr&gt; \"2\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"1\", \"3\", \"3\", \"…\n$ episodeNumber &lt;chr&gt; \"3\", \"4\", \"6\", \"10\", \"4\", \"20\", \"5\", \"2\", \"20\", \"6\", \"2\"…\n\n\nIn this table, we can see the non-standard “null” values are within the seasonNumber and episodeNumber columns. To fix this, the following code will be run:\n\n# Mutate the specified columns\nTITLE_EPISODES &lt;- suppressWarnings(\n  TITLE_EPISODES |&gt;\n    mutate(seasonNumber = as.numeric(seasonNumber),\n           episodeNumber = as.numeric(episodeNumber))\n)"
  },
  {
    "objectID": "mp02.html#title_principals",
    "href": "mp02.html#title_principals",
    "title": "Mini-Project #02",
    "section": "TITLE_PRINCIPALS",
    "text": "TITLE_PRINCIPALS\n\n# Use the glimpse function\nglimpse(TITLE_PRINCIPALS)\n\nRows: 6,586,689\nColumns: 6\n$ tconst     &lt;chr&gt; \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000001\", \"tt0000…\n$ ordering   &lt;dbl&gt; 1, 2, 3, 4, 1, 2, 1, 2, 3, 4, 5, 1, 2, 1, 2, 3, 1, 2, 3, 4,…\n$ nconst     &lt;chr&gt; \"nm1588970\", \"nm0005690\", \"nm0005690\", \"nm0374658\", \"nm0721…\n$ category   &lt;chr&gt; \"self\", \"director\", \"producer\", \"cinematographer\", \"directo…\n$ job        &lt;chr&gt; \"\\\\N\", \"\\\\N\", \"producer\", \"director of photography\", \"\\\\N\",…\n$ characters &lt;chr&gt; \"[\\\"Self\\\"]\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\N\", \"\\\\…\n\n\nIn this table, we can see the non-standard “null” values are within the job and characters columns. To fix this, the following code will be run:\n\n# Mutate the specified columns\nTITLE_PRINCIPALS &lt;- suppressWarnings(\n  TITLE_PRINCIPALS |&gt;\n    mutate(job = na_if(job, \"\\\\N\"),\n           characters = na_if(characters, \"\\\\N\"))\n)"
  },
  {
    "objectID": "mp02.html#title_ratings",
    "href": "mp02.html#title_ratings",
    "title": "Mini-Project #02",
    "section": "TITLE_RATINGS",
    "text": "TITLE_RATINGS\n\n# Use the glimpse function\nglimpse(TITLE_RATINGS)\n\nRows: 372,198\nColumns: 3\n$ tconst        &lt;chr&gt; \"tt0000001\", \"tt0000002\", \"tt0000003\", \"tt0000004\", \"tt0…\n$ averageRating &lt;dbl&gt; 5.7, 5.6, 6.5, 5.4, 6.2, 5.0, 5.4, 5.4, 5.4, 6.8, 5.2, 7…\n$ numVotes      &lt;dbl&gt; 2090, 283, 2094, 184, 2828, 196, 889, 2233, 214, 7699, 3…\n\n\nIn this table, we can see the non-standard “null” values are within the averageRating and numVotes columns. To fix this, the following code will be run:\n\n# Mutate the specified columns\nTITLE_RATINGS &lt;- suppressWarnings(\n  TITLE_RATINGS |&gt;\n    mutate(averageRating = as.numeric(averageRating),\n           numVotes = as.numeric(numVotes))\n)\n\nNow, all of the column types have been fixed, and any non-standard “null” values have been made to be NA."
  },
  {
    "objectID": "mp02.html#simple-analysis",
    "href": "mp02.html#simple-analysis",
    "title": "Mini-Project #02",
    "section": "Simple Analysis",
    "text": "Simple Analysis\nUsing the tables as they are, we can do some simple analyses.\nFirst, we’ll examine the works present.\nWe can quantify the number of movies, TV series, and TV episodes in the data set with the following code:\n\n# Determine the unique values in the titleType column of TITLE_BASICS\nunique(TITLE_BASICS$titleType)\n\n [1] \"short\"        \"movie\"        \"tvSeries\"     \"tvShort\"      \"tvMovie\"     \n [6] \"tvEpisode\"    \"tvMiniSeries\" \"video\"        \"tvSpecial\"    \"videoGame\"   \n\n# Find the number of movies\nmovie_count &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"movie\") |&gt;\n  summarize(count = n())\nprint(movie_count)\n\n# A tibble: 1 × 1\n   count\n   &lt;int&gt;\n1 131662\n\n# Find the number of TV series\nseries_count &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvSeries\") |&gt;\n  summarize(count = n())\nprint(series_count)\n\n# A tibble: 1 × 1\n  count\n  &lt;int&gt;\n1 29789\n\n# Find the number of TV episodes\nepisode_count &lt;- TITLE_BASICS |&gt;\n  filter(titleType == \"tvEpisode\") |&gt;\n  summarize(count = n())\nprint(episode_count)\n\n# A tibble: 1 × 1\n   count\n   &lt;int&gt;\n1 155722\n\n\nWe can determine that there are 131,662 movies, 29,789 TV series, and 155,722 TV episodes in the data set.\nNext, we can determine which long running TV series has the highest average rating with the following code:\n\n# Find the TV series with more than 12 ep + highest average rating\nhighest_rated_title &lt;- TITLE_EPISODES |&gt;\n  filter(episodeNumber &gt;= 12) |&gt;\n  inner_join(TITLE_RATINGS, by = \"tconst\") |&gt;\n  arrange(desc(averageRating)) |&gt;\n  slice(1)\n\n# Get the tconst from the TV series \nhighest_tconst &lt;- highest_rated_title$tconst\n\n# Get the actual TV series title\ncorresponding_title &lt;- TITLE_BASICS |&gt;\n  filter(tconst == highest_tconst)\nprint(corresponding_title)\n\n# A tibble: 1 × 9\n  tconst     titleType primaryTitle      originalTitle isAdult startYear endYear\n  &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;             &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 tt10867894 tvEpisode I challenge the … I challenge …       0      2019      NA\n# ℹ 2 more variables: runtimeMinutes &lt;dbl&gt;, genres &lt;chr&gt;\n\n\nWe can also see that the TV series with more than 12 episodes with the highest average rating is I challenge the Ender Dragon in Minecraft (Ending).\nWhen specifically looking at the TV series Happy Days, a controversial episode aired in 1977, which resulted in a belief that later seasons of the series had lower average ratings than earlier seasons. This specifically refers to:\n\nEarlier seasons: 1974 - 1984\nLater seasons: 2008 - present\n\n\n# Find the tconst of Happy Days (earlier and later)\nHD_early_tconst &lt;- TITLE_BASICS |&gt;  # earlier\n  filter(titleType == \"tvSeries\", \n         primaryTitle == \"Happy Days\",\n         startYear == 1974) |&gt;\n  select(tconst) |&gt;\n  pull()\n\nHD_late_tconst &lt;- TITLE_BASICS |&gt;  # later\n  filter(titleType == \"tvSeries\", \n         primaryTitle == \"Happy Days\",\n         startYear == 2008) |&gt;\n  select(tconst) |&gt;\n  pull()\n\n# Find the average ratings\nHD_early_rating &lt;- TITLE_RATINGS |&gt;  # earlier\n  filter(tconst == HD_early_tconst) |&gt;\n  select(averageRating)\n\nHD_late_rating &lt;- TITLE_RATINGS |&gt;  # later\n  filter(tconst == HD_late_tconst) |&gt;\n  select(averageRating)\n\nprint(HD_early_rating)  # earlier\n\n# A tibble: 1 × 1\n  averageRating\n          &lt;dbl&gt;\n1           7.4\n\nprint(HD_late_rating)  # later\n\n# A tibble: 1 × 1\n  averageRating\n          &lt;dbl&gt;\n1           5.4\n\n\nWe can determine that this belief is true, with the earlier seasons achieving an average rating of 7.4, and the later seasons only having a 5.4.\nSecond, we’ll examine the people present.\nLets say we want to find the oldest living person in the dataset. This is based on a reasonable cutoff birth year of 1920, as some data regarding the death year is missing (for example, Robert De Visée was born in 1655, but has no death year listed).\n\n# Set the cutoff year\ncutoff &lt;- 1920\n\n# Filter for people still alive and born after 1920\noldest_alive &lt;- NAME_BASICS |&gt;\n  filter(!is.na(birthYear) & is.na(deathYear) & birthYear &gt;= cutoff) |&gt;\n  arrange(birthYear) |&gt;\n  slice_head(n=1)\n\n# Print the result\nprint(oldest_alive)\n\n# A tibble: 1 × 6\n  nconst    primaryName  birthYear deathYear primaryProfession knownForTitles   \n  &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;             &lt;chr&gt;            \n1 nm0010114 Lily Aclemar      1920        NA actress           tt0039458,tt0042…\n\n\nWe can determine that the oldest living person in the data set is Lily Aclemar.\nNow, we’re going to look at famous actor Mark Hamill.\n\n# Find the specific IDs for Hamill's works\nmark_titles &lt;- NAME_BASICS |&gt;\n  filter(primaryName == \"Mark Hamill\") |&gt;\n  separate_longer_delim(knownForTitles, \",\") |&gt;\n  pull(knownForTitles)\n\n# Find the corresponding names for the IDs just found\nmark_titles_names &lt;- TITLE_BASICS |&gt;\n  filter(tconst %in% mark_titles) |&gt;\n  select(primaryTitle)\nprint(mark_titles_names)\n\n# A tibble: 4 × 1\n  primaryTitle                                  \n  &lt;chr&gt;                                         \n1 Star Wars: Episode IV - A New Hope            \n2 Star Wars: Episode V - The Empire Strikes Back\n3 Star Wars: Episode VI - Return of the Jedi    \n4 Star Wars: Episode VIII - The Last Jedi       \n\n\nFrom the code, we can determine that the four projects he is most known for are:\n\nStar Wars: Episode IV - A New Hope\nStar Wars: Episode V - The Empire Strikes Back\nStar Wars: Episode VI - Return of the Jedi\nStar Wars: Episode VIII - The Last Jedi\n\nIn these projects, Hamill played the iconic role of Luke Skywalker, a Jedi and son of Anakin Skywalker turned Darth Vader."
  },
  {
    "objectID": "mp02.html#find-a-director",
    "href": "mp02.html#find-a-director",
    "title": "Mini-Project #02",
    "section": "Find a Director",
    "text": "Find a Director\nFirst, we must find a director for the film.\n\n# Combine TITLE_BASICS and TITLE_RATINGS on \"tconst\"\ncombo &lt;- TITLE_RATINGS |&gt;\n  left_join(TITLE_BASICS, by = \"tconst\")\n\n# Filter for successful action movies\naction_movies &lt;- combo |&gt;\n  filter(titleType == \"movie\", \n         genres == \"Action\", \n         success == TRUE)\n\n# Separate + merge to get director information\ndirectors &lt;- NAME_BASICS |&gt;\n  separate_longer_delim(primaryProfession, \",\") |&gt;\n  filter(primaryProfession == \"director\") |&gt;\n  select(primaryName, knownForTitles)\n\n# Separate knownForTitles in directors dataset\ndirectors &lt;- directors |&gt;\n  separate_longer_delim(knownForTitles, \",\")\n\n# Join the action movies with directors to find top directors\ntop_directors &lt;- action_movies |&gt;\n  left_join(directors, \n            by = c(\"tconst\" = \"knownForTitles\")) |&gt;\n  group_by(primaryName, tconst, primaryTitle, averageRating, numVotes) |&gt;\n  summarize(success_count = n(), \n            .groups = \"drop\") |&gt;\n  arrange(desc(success_count)) |&gt;\n  slice_head(n = 5)\n\n# Print the top 5 directors for action movies\nprint(top_directors)\n\n# A tibble: 1 × 6\n  primaryName  tconst     primaryTitle averageRating numVotes success_count\n  &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;         &lt;int&gt;\n1 Sonia Bhatia tt14056466 Yudhra                 8.1    11139             1\n\n\nBased on this code, we will select Sonia Bhatia as the director of this film. She previously worked on Yudhra, which received an impressive average IMDb rating of 8.1, out of 11,139 ratings."
  },
  {
    "objectID": "mp02.html#find-2-actors",
    "href": "mp02.html#find-2-actors",
    "title": "Mini-Project #02",
    "section": "Find 2 Actors",
    "text": "Find 2 Actors\nInstead of finding actors with extensive experience in successful action movies, we will look at actors that are well known; that is, actors that have appeared in a minimum of 3 successful movies. This would certainly spark intrigue in the public, bringing more attention to the film.\n\n# Combine TITLE_BASICS and TITLE_RATINGS on \"tconst\"\ncombo &lt;- TITLE_RATINGS |&gt;\n  left_join(TITLE_BASICS, by = \"tconst\")\n\n# Filter for successful movies\nsuccessful_movies &lt;- combo |&gt;\n  filter(success == TRUE)\n\n# Separate actors in NAME_BASICS\nactors &lt;- NAME_BASICS |&gt;\n  separate_longer_delim(primaryProfession, \",\") |&gt;\n  filter(primaryProfession %in% c(\"actor\", \"actress\")) |&gt;\n  select(nconst, primaryName, knownForTitles)\n\n# Separate knownForTitles for each actor\nactors &lt;- actors |&gt;\n  separate_longer_delim(knownForTitles, \",\")\n\n# Join the successful movies with actors\nactor_movies &lt;- successful_movies |&gt;\n  left_join(actors, by = c(\"tconst\" = \"knownForTitles\"))\n\n# Movie actors\nmovie_actors &lt;- actor_movies |&gt;\n  filter(titleType == \"movie\")\n\n# Count # of successful movies for each actor\nactor_success_count &lt;- movie_actors |&gt;\n  group_by(nconst, primaryName) |&gt;\n  summarize(success_count = n(), \n            .groups = \"drop\")\n\n# Filter for actors who have appeared in at least 3 successful movies\nactors_with_3_successes &lt;- actor_success_count |&gt;\n  filter(success_count &gt;= 3) |&gt;\n  arrange(desc(success_count))\n\n# Make a table\ndatatable(actors_with_3_successes)\n\n\n\n\n\nThis code displays all the actors that have appeared in at least 3 successful movies, in descending order, in a table.\nWe will only be selecting 2; therefore, we will run a code that will display only the top 5 actors within this table in a graph, and select from there.\n\n# Select the top 5 actors\ntop_5_actors &lt;- actors_with_3_successes |&gt;\n  slice_head(n = 5)\n\n# Bar graph\nggplot(top_5_actors, \n       aes(x = reorder(primaryName, success_count), \n           y = success_count, \n           fill = primaryName)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Top 5 Actors with Most Successful Movies\",\n       x = \"Actor\",\n       y = \"Number of Successful Movies\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nThe top candidate doesn’t count, as it is just the compilation of all the unnamed actors that meet the criteria. Following this is Charles Chaplin, who is unfortunately dead and will not be able to be reached through non-paranormal means.\nAfter all of this analysis, we will select Kevin Spacey and Elijah Wood to star in the film."
  },
  {
    "objectID": "mp02.html#pitch-the-personnel",
    "href": "mp02.html#pitch-the-personnel",
    "title": "Mini-Project #02",
    "section": "Pitch the Personnel",
    "text": "Pitch the Personnel\nKevin Spacey and Elijah Wood would form an intriguing duo for this action movie, blending Spacey’s intense, nuanced performances with Wood’s unique charm and versatility. Spacey is renowned for his ability to portray morally complex characters, while Wood brings youthful energy and a knack for playing quirky, relatable roles, making him the ideal counterpart. Together, they promise to create dynamic on-screen chemistry that elevates the story. With director Sonia Bhatia’s fresh perspective, the film is set to deliver a visually stunning and engaging narrative."
  },
  {
    "objectID": "mp02.html#select-the-movie",
    "href": "mp02.html#select-the-movie",
    "title": "Mini-Project #02",
    "section": "Select the Movie",
    "text": "Select the Movie\nFirst, we’re going to look at all of the successful action movies to date. We will also remove any remakes; we want to be the first remake of the classic we select.\n\n# Merge TITLE_BASICS and TITLE_RATINGS on \"tconst\"\ncombo &lt;- TITLE_RATINGS |&gt;\n  left_join(TITLE_BASICS, by = \"tconst\")\n\n# Separate genres\ncombo3 &lt;- combo |&gt;\n  separate_longer_delim(genres, \",\")\n\n# Filter for successful action movies\nsuccessful_action_movies &lt;- combo3 |&gt;\n  filter(success == TRUE, \n         titleType == \"movie\", \n         genres %in% \"Action\") |&gt;\n  distinct(primaryTitle, .keep_all = TRUE) # Keep first occurrence of each title\n\n# Make a datatable\nsuccessful_action_movies &lt;- successful_action_movies |&gt;\n  select(tconst, startYear, primaryTitle, averageRating)\ndatatable(successful_action_movies)\n\n\n\n\n\nThis table shows all unique, successful action movies in the dataset.\nWe will now narrow the scope to decide on a movie. We will define a classic movie as follows:\n\nMade prior to 1999\nAt least 500,000 ratings\n\nHowever, for our purposes, we’ll stay within the 1990s, just so we can appeal more towards late Millenials and early Gen Z who want to see a twist on a classic from their childhood. These generations are the largest on Earth, pointing towards a substantial target audience.\n\n# Merge TITLE_BASICS and TITLE_RATINGS on \"tconst\"\ncombo &lt;- TITLE_RATINGS |&gt;\n  left_join(TITLE_BASICS, by = \"tconst\")\n\n# Separate the genres\ncombo2 &lt;- combo |&gt;\n  separate_longer_delim(genres, \",\")\n\n# Filter\nclassic_movie &lt;- combo2 |&gt;\n  filter(success == TRUE, \n         titleType == \"movie\", \n         genres == \"Action\", \n         between(startYear, 1990, 1999),      \n         numVotes &gt;= 500000) \n\n# Make the datatable\nclassic_movie &lt;- classic_movie |&gt;\n  select(tconst, startYear, primaryTitle, averageRating, numVotes) |&gt;\n  arrange(desc(numVotes))\ndatatable(classic_movie)\n\n\n\n\n\nNow that we’ve narrowed it down to 6 classics, we will select the movie with the most ratings, which is The Matrix."
  },
  {
    "objectID": "mp02.html#look-into-the-personnel-from-that-movie",
    "href": "mp02.html#look-into-the-personnel-from-that-movie",
    "title": "Mini-Project #02",
    "section": "Look Into the Personnel from that Movie",
    "text": "Look Into the Personnel from that Movie\nFor legal purposes, we must check to see if key actors, directors, or writers from The Matrix are still alive. If they are, we’ll need to ensure we can secure the rights to the project.\n\n# Establish The Matrix\ntop_movie &lt;- classic_movie |&gt;\n  arrange(desc(numVotes)) |&gt;\n  slice(1)\n\n# Grab The Matrix' tconst\ntop_movie_tconst &lt;- top_movie$tconst\n\n# Find associated people\nassociated_people &lt;- TITLE_PRINCIPALS |&gt;\n  filter(tconst == top_movie_tconst) |&gt;\n  select(nconst)\n\n# Filter for people still alive\nalive_people &lt;- NAME_BASICS |&gt;\n  filter(nconst %in% associated_people$nconst,\n         is.na(deathYear)) |&gt;\n  select(nconst, primaryName)\nprint(alive_people)\n\n# A tibble: 17 × 2\n   nconst    primaryName       \n   &lt;chr&gt;     &lt;chr&gt;             \n 1 nm0000206 Keanu Reeves      \n 2 nm0000401 Laurence Fishburne\n 3 nm0001592 Joe Pantoliano    \n 4 nm0005251 Carrie-Anne Moss  \n 5 nm0005428 Joel Silver       \n 6 nm0032810 Julian Arahanga   \n 7 nm0159059 Marcus Chong      \n 8 nm0204485 Don Davis         \n 9 nm0233391 Matt Doran        \n10 nm0565883 Belinda McClory   \n11 nm0665517 Owen Paterson     \n12 nm0691084 Bill Pope         \n13 nm0821205 Zach Staenberg    \n14 nm0905152 Lilly Wachowski   \n15 nm0905154 Lana Wachowski    \n16 nm0915989 Hugo Weaving      \n17 nm0938441 Shauna Wolifson   \n\n\nWe can see that 17 are still alive, so we will need legal counsel.\nIt would also be fun to include actors from the original movie in the project as a callback for the fans.\n\nassociated_actors &lt;- TITLE_PRINCIPALS |&gt;\n  filter(tconst == top_movie_tconst, \n         category %in% c(\"actor\", \"actress\")) |&gt;\n  select(nconst)\n\nalive_actors &lt;- NAME_BASICS |&gt;\n  filter(nconst %in% associated_actors$nconst,\n         is.na(deathYear)) |&gt;\n  select(nconst, primaryName)\nprint(alive_actors)\n\n# A tibble: 9 × 2\n  nconst    primaryName       \n  &lt;chr&gt;     &lt;chr&gt;             \n1 nm0000206 Keanu Reeves      \n2 nm0000401 Laurence Fishburne\n3 nm0001592 Joe Pantoliano    \n4 nm0005251 Carrie-Anne Moss  \n5 nm0032810 Julian Arahanga   \n6 nm0159059 Marcus Chong      \n7 nm0233391 Matt Doran        \n8 nm0565883 Belinda McClory   \n9 nm0915989 Hugo Weaving      \n\n\nBased on this code, we will try to request for Keanu Reeves and Laurence Fishburne to have a cameo role of sorts in our project."
  },
  {
    "objectID": "mp02.html#the-elevator-pitch",
    "href": "mp02.html#the-elevator-pitch",
    "title": "Mini-Project #02",
    "section": "The Elevator Pitch",
    "text": "The Elevator Pitch\nWe have an opportunity to capitalize on the surging demand for action films, which have seen a remarkable rise in success rates—up from just 0.2% overall to a solid 1.34% in the last decade. This genre is proving its staying power, especially with the right mix of talent and vision.\nAt the helm, we propose acclaimed director Sonia Bhatia, who recently delivered the highest IMDb-rated action movie of the past decade, Yudhra. Her distinct style combines cutting-edge visual storytelling with character-driven action sequences that resonate with today’s audiences.\nFor our cast, we’ll bring in two powerhouses: Kevin Spacey and Elijah Wood. Both actors have consistently delivered in the box office, each starring in at least three successful films. Their ability to bring depth and nuance to complex roles will elevate this project, especially in a high-concept action setting.\nThe story will draw inspiration from the best-rated action movie on IMDb, The Matrix. We aim to replicate the groundbreaking impact of that film by blending high-octane action with a mind-bending narrative. The unique combination of talent and proven story appeal gives us a strong edge in the marketplace.\nWith this package, we have the perfect formula for a modern blockbuster that both excites audiences and guarantees returns."
  },
  {
    "objectID": "mp02.html#the-trailer",
    "href": "mp02.html#the-trailer",
    "title": "Mini-Project #02",
    "section": "The Trailer",
    "text": "The Trailer\nThe marketing department has requested a classic 90’s style teaser for them to create a brief trailer for the next quarterly earnings call. We’ll now send them the following:\n“From director Sonia Bhatia, the visionary mind behind Yudhra; and from actors Kevin Spacey, Hollywood icon and beloved star of American Beauty, and Elijah Wood from The Lord of the Rings comes the timeless tale of The Matrix, a story of dystopian realities, relentless pursuit, and cutting-edge action. Coming soon to a theater near you!”\nThe sample movie poster can be found here."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #03",
    "section": "",
    "text": "Author: Thanh Dao\nUpdated: November 12th, 2024 @ 7:42PM"
  },
  {
    "objectID": "mp03.html#r-packages",
    "href": "mp03.html#r-packages",
    "title": "Mini-Project #03",
    "section": "R Packages",
    "text": "R Packages\nFirst, we will download the necessary R packages that will be used throughout the report.\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(sf)"
  },
  {
    "objectID": "mp03.html#data-1-us-house-election-votes-1976---2022",
    "href": "mp03.html#data-1-us-house-election-votes-1976---2022",
    "title": "Mini-Project #03",
    "section": "Data 1: US House Election Votes (1976 - 2022)",
    "text": "Data 1: US House Election Votes (1976 - 2022)\nThe first data source we will use is the MIT Election Data Science Lab, which collects votes from all biennial congressional races in all 50 states, as well as the statewide presidential vote counts.\nThe data will be downloaded manually as csv files through a web browser, due to it requiring the user’s contact information.\n\n\nCode\n# Load the biennial congressional races data\ncongress_races &lt;- readr::read_csv(\"1976-2022-house.csv\")\n\n# Load the presidential races data\npres_races &lt;- readr::read_csv(\"1976-2020-president.csv\")"
  },
  {
    "objectID": "mp03.html#data-2-congressional-boundary-files-1976---2012",
    "href": "mp03.html#data-2-congressional-boundary-files-1976---2012",
    "title": "Mini-Project #03",
    "section": "Data 2: Congressional Boundary Files (1976 - 2012)",
    "text": "Data 2: Congressional Boundary Files (1976 - 2012)\nWe will now automate the download process for zip files for all US Congressional districts from 1976 to 2012. They will be obtained from here.\n\n\nCode\nget_file &lt;- function(fname){\n  BASE_URL &lt;- \"https://cdmaps.polisci.ucla.edu/shp/\"\n  fname_ext &lt;- paste0(fname, \".zip\")\n  fname_ext1 &lt;- paste0(fname, \".shp\")\n  fname_extunzip &lt;- gsub(\".zip$\", \"\", fname_ext)\n  subfolder &lt;- \"districtShapes\"\n  if(!file.exists(fname_ext)){\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL, \n                  destfile = fname_ext)\n  }\n  # Unzip contents and save\n  unzip(zipfile = fname_ext, exdir = fname_extunzip)\n  # Define file Path\n  shapefile_path &lt;- file.path(fname_extunzip, subfolder, fname_ext1)\n  # Read .shp\n  read_sf(shapefile_path)\n}\n\n# Download files by iterating through\nstart_congress = 95\nend_congress = 114\nfor (i in start_congress:end_congress) {\n  district_name &lt;- sprintf(\"districts%03d\", i) \n  district_data &lt;- get_file(district_name)\n  assign(district_name, district_data, envir = .GlobalEnv) \n}"
  },
  {
    "objectID": "mp03.html#data-3-congressional-boundary-files-2014---present",
    "href": "mp03.html#data-3-congressional-boundary-files-2014---present",
    "title": "Mini-Project #03",
    "section": "Data 3: Congressional Boundary Files (2014 - Present)",
    "text": "Data 3: Congressional Boundary Files (2014 - Present)\nNow, we will automate the download process for US Census Bureau shp from here; specifically the CD directory found in the FTP Archive.\n\n\nCode\nget_congress_file &lt;- function(fname, year){\n  BASE_URL &lt;- sprintf(\"https://www2.census.gov/geo/tiger/TIGER%d/CD/\", year)\n  fname_ext &lt;- paste0(fname, \".zip\")\n  fname_ext1 &lt;- paste0(fname, \".shp\")\n  fname_extunzip &lt;- gsub(\".zip$\", \"\", fname_ext)\n  \n  # Download File\n  if(!file.exists(fname_ext)){\n    FILE_URL &lt;- paste0(BASE_URL, fname_ext)\n    download.file(FILE_URL, \n                  destfile = fname_ext)\n  }\n  # Unzip contents and save\n  unzip(zipfile = fname_ext, exdir = fname_extunzip)\n  # Define File Path\n  shapefile_path &lt;- file.path(fname_extunzip, fname_ext1)\n  # Read .shp\n  read_sf(shapefile_path)\n}\n\n# Download file for each district by iterating through each year\nbase_year = 2022\nbase_congress = 116  # Congress number for 2012\nfor (i in 0:10) { \n  year &lt;- base_year - i\n  if (year &gt;= 2018) {congress &lt;- 116} \n  else if (year &gt;= 2016) {congress &lt;- 115} \n  else if (year &gt;= 2014) {congress &lt;- 114} \n  else if (year == 2013) {congress &lt;- 113} \n  else if (year == 2012) {congress &lt;- 112}\n  district_name &lt;- sprintf(\"tl_%d_us_cd%d\", year, congress)\n  district_data &lt;- get_congress_file(district_name, year) \n  assign(district_name, district_data, envir = .GlobalEnv)  \n  }"
  },
  {
    "objectID": "mp03.html#seats-in-the-us-house-of-representatives",
    "href": "mp03.html#seats-in-the-us-house-of-representatives",
    "title": "Mini-Project #03",
    "section": "Seats in the US House of Representatives",
    "text": "Seats in the US House of Representatives\nFirst, we’ll analyze the seats in the US House of Representatives. Specifically, we want to look at which states have gained and lost the most seats between 1976 and 2022.\n\n\nCode\n# Only look at the years 1976 and 2022\nfiltered_c &lt;- congress_races |&gt;\n  filter(year %in% c(1976, 2022))\n\n# Count the number of unique candidates / state / year\nseats_by_state &lt;- filtered_c |&gt;\n  group_by(year, state) |&gt;\n  summarize(seats = n_distinct(candidate), \n            .groups = \"drop\")\n\n# Add a column for seats in 1976 + 2022\nseats_c &lt;- seats_by_state |&gt;\n  pivot_wider(names_from = year, \n              values_from = seats,\n              names_prefix = \"seats_\")\n\n# Calculate change in # of seats\nseats_c &lt;- seats_c |&gt;\n  mutate(seat_change = seats_2022 - seats_1976)\n\n# States with most gained / lost seats\ngained_most &lt;- seats_c |&gt;\n  filter(seat_change == max(seat_change, \n                            na.rm = TRUE))\nlost_most &lt;- seats_c |&gt;\n  filter(seat_change == min(seat_change,\n                            na.rm = TRUE))\n\n# Data tables\ndatatable(setNames(gained_most,\n                   c(\"State\", \"1976 Seats\", \"2022 Seats\", \"Difference\")))\n\n\n\n\n\n\nCode\ndatatable(setNames(lost_most,\n                   c(\"State\", \"1976 Seats\", \"2022 Seats\", \"Difference\")))\n\n\n\n\n\n\nWe can see that Florida gained the most seats, with it having 27 seats in 1976 and 72 in 2022, pointing to a seat change of \\(45\\). On the other hand, New York lost the most seats, with it having 110 seats in 1976 and 58 in 2022, pointing to a seat change of \\(-52\\)."
  },
  {
    "objectID": "mp03.html#the-fusion-voting-system",
    "href": "mp03.html#the-fusion-voting-system",
    "title": "Mini-Project #03",
    "section": "The “Fusion” Voting System",
    "text": "The “Fusion” Voting System\nNew York uses a unique voting system called fusion voting. Fusion voting is a system where one candidate can appear on multiple lines of the ballot, and all of these votes are totaled. This system has changed the outcome of many elections, compared to if candidates only received the votes from their major party lines, rather than the total number of votes across all lines.\nWe’re going to find which elections would have had a different result.\n\n\nCode\n# Filter for NY\nny_elections &lt;- congress_races |&gt;\n  filter(state == \"NEW YORK\")\n\n# Identify major party votes\nny_major_party_votes &lt;- ny_elections |&gt;\n  mutate(major_party_line = case_when(\n    party == \"DEMOCRAT\" ~ \"DEMOCRAT\",\n    party == \"REPUBLICAN\" ~ \"REPUBLICAN\",\n    TRUE ~ \"OTHER\" # NA or other parties\n  )) |&gt;\n  filter(major_party_line != \"OTHER\") |&gt; # No minor parties\n  group_by(year, candidate, major_party_line) |&gt; # Sum major\n  summarize(major_party_votes = sum(candidatevotes),\n            .groups = \"drop\")\n\n# Merge with total votes\nelection_results &lt;- ny_elections |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes), \n            .groups = \"drop\") |&gt;\n  left_join(ny_major_party_votes, \n            by = c(\"year\", \"candidate\"))\n\n# Elections with diff outcome\nchanged_outcomes &lt;- election_results |&gt;\n  group_by(year) |&gt; \n  # Rank by actual total votes\n  arrange(desc(total_votes)) |&gt; \n  # Mark actual winner\n  mutate(actual_winner = candidate == first(candidate)) |&gt; \n  # Rank by major party votes only\n  arrange(desc(major_party_votes)) |&gt; \n  # Mark hypothetical winner\n  mutate(hypothetical_winner = candidate == first(candidate)) |&gt;\n  # Filter for diff outcomes\n  filter(actual_winner != hypothetical_winner) |&gt;\n  select(year, \n         candidate, \n         total_votes, \n         major_party_votes, \n         actual_winner, \n         hypothetical_winner)\n\n# Data tables\ndatatable(setNames(changed_outcomes,\n                   c(\"Year\", \"Candidate\", \"Total Votes\",\n                     \"Major Party Votes\", \"Won?\", \"Would Have Won?\")),\n          options = list(pageLength = 10, \n                         autoWidth = TRUE))\n\n\n\n\n\n\nThis table displays the elections where fusion voting changed the outcome. The candidates shown did not win the actual election outcome, but would have won if only their votes from the major party line were counted."
  },
  {
    "objectID": "mp03.html#presidential-vs-congressional-candidates",
    "href": "mp03.html#presidential-vs-congressional-candidates",
    "title": "Mini-Project #03",
    "section": "Presidential vs Congressional Candidates",
    "text": "Presidential vs Congressional Candidates\nIt would be interesting to analyze whether presidential candidates tend to run ahead of or behind congressional candidates from the same party in each state, which can be done with the following code:\n\n\nCode\n# Aggregate congressional votes by state, year, party\ncongress_agg &lt;- congress_races |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(total_congress_votes = sum(candidatevotes),\n            .groups = \"drop\")\n\n# Filter for Dem / Rep for simplicity's sake\npres_races_filtered &lt;- pres_races |&gt;\n  filter(party_simplified %in% c(\"DEMOCRAT\",\n                                 \"REPUBLICAN\"))\n\n# Join by year, state, party\ncombined_data &lt;- pres_races_filtered |&gt;\n  inner_join(congress_agg, by = c(\"year\",\n                                  \"state\",\n                                  \"party_simplified\" = \"party\")) |&gt;\n  mutate(vote_difference = candidatevotes - total_congress_votes) |&gt;\n  select(\"year\", \n         \"state\", \n         \"candidate\", \n         \"candidatevotes\", \n         \"totalvotes\", \n         \"party_simplified\", \n         \"total_congress_votes\", \n         \"vote_difference\")\n\n# Data tables\ndatatable(setNames(combined_data,\n                   c(\"Year\", \"State\", \"Candidate\", \n                     \"# of Votes\", \"Total # of Votes\",\n                     \"Party\", \"Congress Votes\", \"Difference\")),\n          options = list(pageLength = 10, \n                         autoWidth = TRUE))\n\n\n\n\n\n\nThis table shows if a presidential candidate got more or less votes in a given state than all of the congressional candidates in their party and the state. If the number under Difference is positive, the presidential candidate had more; if it is negative, the presidential candidate had less.\n\nTrend over Time\nWe can now see if presidential candidates have tended to run ahead or behind congressional candidates historically.\n\n\nCode\n# Average trend over time\ntrend_over_time &lt;- combined_data |&gt;\n  group_by(year) |&gt;\n  summarize(avg_vote_diff = mean(vote_difference,\n                                       na.rm = TRUE))\n\n# Line graph\nggplot(trend_over_time,\n       aes(x = year,\n           y = avg_vote_diff)) +\n  geom_line(color = \"blue\") +\n  geom_smooth(method = \"lm\",\n              color = \"red\",\n              se = FALSE) +\n  labs(title = \"Trend of Presidential Vote Share vs Congressional Vote Share Over Time\",\n       x = \"Year\",\n       y = \"Average Vote Difference\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe average vote difference is somewhat all over the place; however, with the linear line displayed, we can see there is a slight upward trend. This suggests that over time, presidential candidates are generally receiving a higher share of votes relative to their congressional counterparts in the same state and party.\n\n\nTrend by State\nWe’re now going to observe if the trend differs by state.\n\n\nCode\n# Trend by state + party\ntrend_by_state &lt;- combined_data |&gt;\n  group_by(state) |&gt;\n  summarize(avg_vote_difference = round(mean(vote_difference,\n                                       na.rm = TRUE), 3)) |&gt;\n  arrange(desc(avg_vote_difference))\n\n# Data table\ndatatable(setNames(trend_by_state,\n                   c(\"State\", \"Average Difference\")),\n          options = list(pageLength = 10,\n                         autoWidth = TRUE))\n\n\n\n\n\n\nOut of the states, Florida has the greatest average vote difference, with \\(426,701.292\\) votes. Maryland has the least, with \\(-110466.296\\) votes. This points to the presidential candidates in Florida tending to receive more votes than their congressional counterparts. In Maryland, the opposite is true, where they receive less votes than their congressional counterparts.\n\n\nTrend by Party\nWe’re now going to observe if the trend differs by party.\n\n\nCode\n# Trend by state + party\ntrend_by_party &lt;- combined_data |&gt;\n  group_by(party_simplified) |&gt;\n  summarize(avg_vote_differences = round(mean(vote_difference,\n                                       na.rm = TRUE), 3)) |&gt;\n  arrange(desc(avg_vote_differences))\n\n# Data table\ndatatable(setNames(trend_by_party,\n                   c(\"Party\", \"Average Difference\")))\n\n\n\n\n\n\nRepublicans have an overwhelming large difference in average votes, with \\(88,082.336\\) votes, compared to Democrats, with \\(22,697.968\\) votes. This suggests that Republican presidential candidates are generally receiving a higher share of votes relative to their congressional counterparts.\n\n\nOutliers\nWe’re now going to see if there were any presidential candidates who were especially popular or unpopular, relative to their party’s congressional candidates.\n\n\nCode\n# Group by presidential candidate, calculate avg vote diff from congress\npres_pop &lt;- combined_data |&gt;\n  group_by(year, candidate, party_simplified) |&gt;\n  summarize(av_vote_diffs = round(mean(vote_difference, na.rm = TRUE), 3)) |&gt;\n  ungroup() |&gt;\n  slice_max(order_by = av_vote_diffs, n = 5)\n\n# Data table\ndatatable(setNames(pres_pop,\n                   c(\"Year\", \"Candidate\", \"Party\", \"Average Difference\")),\n          options = list(pageLength = 10, autoWidth = TRUE))\n\n\n\n\n\n\nThis table displays the top 5 most popular presidential candidates, based on the average vote difference compared to congressional candidates. The top candidate is Ronald Reagan, with \\(311,941.04\\) votes."
  },
  {
    "objectID": "mp03.html#state-wide-winner-take-all",
    "href": "mp03.html#state-wide-winner-take-all",
    "title": "Mini-Project #03",
    "section": "1. State-Wide Winner-Take-All",
    "text": "1. State-Wide Winner-Take-All\nIn a state-wide winner-take-all strategy, the state will award all \\(R + 2\\) electoral college votes to the winner of their state-wide popular vote. Basically, whichever candidate receives the most votes in the state wins all the electoral votes.\n\n\nCode\n# Candidate with the most votes each year in each state\nstate_winner_take_all &lt;- pres_races |&gt;\n  group_by(year,\n           state,\n           candidate) |&gt;\n  summarize(total_votes = sum(candidatevotes),\n            .groups = \"drop\") |&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by = total_votes,\n            n=1,\n            with_ties = FALSE) |&gt;\n  rename(winner = candidate)\n\n# Find which candidate gest the most electoral votes\nstate_winner_take_all &lt;- state_winner_take_all |&gt;\n  left_join(ecv_by_state,\n            by = c(\"year\", \"state\")) |&gt;\n  group_by(year, winner) |&gt;\n  summarize(total_ecv = sum(ecv)) |&gt;\n  slice_max(order_by = total_ecv,\n            n = 1,\n            with_ties = FALSE)\n\n# Data table\ndatatable(setNames(state_winner_take_all, \n                   c(\"Year\", \"Winning Candidate\", \"ECVs\")),\n          options = list(pageLength = 10,\n                         autoWidth = TRUE))"
  },
  {
    "objectID": "mp03.html#district-wide-winner-take-all-and-state-wide-at-large-votes",
    "href": "mp03.html#district-wide-winner-take-all-and-state-wide-at-large-votes",
    "title": "Mini-Project #03",
    "section": "2. District-Wide Winner-Take-All and State-Wide “At Large” Votes",
    "text": "2. District-Wide Winner-Take-All and State-Wide “At Large” Votes\nIn a district-wide winner-take-all strategy, each district electoral vote is given to the district winner, and the candidate that wins the popular vote in the state gets the remaining 2.\n\n\nCode\n# Find # of districts each party won\n# This represents ECV won in each state\ndistrict_winner &lt;- congress_races |&gt;\n  group_by(year, state, district) |&gt;\n  slice_max(order_by = candidatevotes,\n            n = 1,\n            with_ties = FALSE) |&gt;\n  select(year, \n         state, \n         district, \n         party) |&gt;\n  group_by(year, state, party) |&gt;\n  summarize(districts_won = n())\n\n# Popular vote winner in the state\npop_winner &lt;- congress_races |&gt;\n  group_by(year, state) |&gt;\n  slice_max(order_by = candidatevotes,\n            n = 1,\n            with_ties = FALSE) |&gt;\n  select(year, state, party) |&gt;\n  add_column(pop_votes = 2)\n\n# Total ECV presidential party received in the state\ndistrict_wide_winner &lt;- district_winner |&gt;\n  left_join(pop_winner,\n            by = c(\"year\", \"state\", \"party\")) |&gt;\n  mutate(across(where(is.numeric), \n                ~ ifelse(is.na(.), 0, .))) |&gt;\n  mutate(total_electoral = districts_won + pop_votes) |&gt;\n  select(-districts_won, -pop_votes) |&gt;\n  rename(party_simplified = party) |&gt;\n  left_join(pres_races,\n            by = c(\"year\", \"state\", \"party_simplified\")) |&gt;\n  select(year, state, total_electoral, candidate) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(electoral = sum(total_electoral)) |&gt;\n  slice_max(order_by = electoral,\n            n = 1,\n            with_ties = FALSE) |&gt;\n  drop_na()\n\n# Data table\ndatatable(setNames(district_wide_winner, \n                   c(\"Year\", \"Winning Candidate\", \"Electoral Votes\")),\n          options = list(pageLength = 10, autoWidth = TRUE))"
  },
  {
    "objectID": "mp03.html#state-wide-proportional",
    "href": "mp03.html#state-wide-proportional",
    "title": "Mini-Project #03",
    "section": "3. State-Wide Proportional",
    "text": "3. State-Wide Proportional\nIn a state-wide proportional strategy, the electoral college votes are distributed based on the percentage of the popular vote that the candidate received in the state.\nThe following code will get the winning candidate per state per year.\n\n\nCode\n# Calculate vote share per candidate in each state / year\npres_races1 &lt;- pres_races |&gt;\n  mutate(vote_share = round((candidatevotes / totalvotes), digits = 0))\n\n# Join ecv data and pres races data\npres_races1 &lt;- pres_races1 |&gt;\n  left_join(ecv_by_state,\n            by = c(\"year\", \"state\"))\n\n# Calculate ECVs per candidate\npres_races1 &lt;- pres_races1 |&gt;\n  mutate(ecv_per_candidate = round((vote_share * ecv), digits = 0))\n\n# Filter to keep only the candidate with the maximum ECVs per state and year\nstate_proportion &lt;- pres_races1 |&gt;\n  group_by(year, state) |&gt;\n  slice_max(ecv_per_candidate, with_ties = FALSE) |&gt;\n  ungroup()\n\n# Select final columns for display\nstate_proportion &lt;- state_proportion |&gt;\n  select(year, state, candidate, ecv_per_candidate)\n\n\nNow that we have that information, we can determine the presidential winner based on the state-wide-proportional strategy.\n\n\nCode\n# Find national winner\nnational_winner &lt;- state_proportion |&gt;\n  group_by(year, candidate) |&gt;\n  summarise(total_ecv = sum(ecv_per_candidate, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  group_by(year) |&gt;\n  slice_max(total_ecv, with_ties = FALSE) |&gt;\n  ungroup()\n\n# Display the national winner for each year\ndatatable(\n  setNames(national_winner, c(\"Year\", \"Candidate\", \"Total Electoral Votes\")),\n  options = list(pageLength = 10, autoWidth = TRUE)\n)"
  },
  {
    "objectID": "mp03.html#national-proportional",
    "href": "mp03.html#national-proportional",
    "title": "Mini-Project #03",
    "section": "4. National Proportional",
    "text": "4. National Proportional\nIn a national-proportional strategy, the electoral college votes are distributed based on the percentage of the popular vote that the candidate received in the nation.\n\n\nCode\n# # of ECV available\necv_available &lt;- ecv_by_state |&gt;\n  group_by(year) |&gt;\n  summarize(ecv_vote = sum(ecv))\n\n# % of popular vote each candidate received\nnat_prop &lt;- pres_races |&gt;\n  select(year, state, candidate, candidatevotes) |&gt;\n  group_by(year, candidate) |&gt;\n  summarize(total_electoral_votes = sum(candidatevotes)) |&gt;\n  group_by(year) |&gt;\n  mutate(pop_vote_count = sum(total_electoral_votes)) |&gt;\n  ungroup() |&gt;\n  mutate(perc_pop_vote = (total_electoral_votes / pop_vote_count)) |&gt;\n  select(-total_electoral_votes, -pop_vote_count) |&gt;\n  left_join(ecv_available,\n            join_by(year == year)) |&gt;\n  mutate(ecv_received = round(perc_pop_vote * ecv_vote, digits = 0)) |&gt;\n  select(-perc_pop_vote, -ecv_vote) |&gt;\n  group_by(year) |&gt;\n  slice_max(order_by = ecv_received,\n            n = 1,\n            with_ties = FALSE) |&gt;\n  rename(winner = candidate)\n\n# Data table\ndatatable(setNames(nat_prop,\n          c(\"Year\", \n            \"Winning Candidate\",\n            \"ECV\")),\n          options = list(pageLength = 10,\n                         autoWidth = TRUE))"
  },
  {
    "objectID": "mp03.html#the-fact-check",
    "href": "mp03.html#the-fact-check",
    "title": "Mini-Project #03",
    "section": "The Fact Check",
    "text": "The Fact Check\nThere are pros and cons to all of these electoral college vote allocation strategies However, after analyzing all of the data, the national proportional strategy appears to be the “fairest,” due to it being representative of the entire nation’s voting population’s interests. It takes into account the people’s opinions evenly, irrespective to the state populations.\nFollowing this, the state-wide proportional strategy would be considered “second fairest.” It is representative of the people’s opinions on a state level; this is what makes it “less fair” than the national proportional strategy, as people’s opinions are weighed by state. There is a potential for skews in opinion due to each state having a different amount of electoral college votes available.\nThe state-wide winner-take-all and district-wide winner-take-all and state-wide “at large” votes strategies are the “least fair” strategies available, due to them not offering a true representation of the nation’s interests. The state-strategy only represents a portion of the sentiments of the voters in a state, and the district-wide strategy is prone to skewness from different population sizes within the district. This is different from the state-wide proportional and national proportional, due to the difference in population size scaling."
  },
  {
    "objectID": "test.html",
    "href": "test.html",
    "title": "Untitled",
    "section": "",
    "text": "Testing Document"
  },
  {
    "objectID": "individual_report.html",
    "href": "individual_report.html",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "",
    "text": "Author: Thanh Dao\nLast Updated: December 18th, 2024 @2:33PM"
  },
  {
    "objectID": "individual_report.html#r-packages",
    "href": "individual_report.html#r-packages",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "R Packages",
    "text": "R Packages\nFirst, we will load the necessary R packages that will be used throughout the report.\n\n\nCode\nlibrary(tidyverse)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(lubridate)\nlibrary(sf)\nlibrary(corrplot)"
  },
  {
    "objectID": "individual_report.html#data-sources",
    "href": "individual_report.html#data-sources",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Data Sources",
    "text": "Data Sources\nSecond, we will download the necessary data sources. Namely, we will be downloading the following:\n\nRat Sightings from NYC Open Data\nRodent Inspection from NYC Open Data\n\n\n\nCode\nrat_sightings &lt;- suppressWarnings(readr::read_csv(\"rat_sightings.csv\"))\nrat_inspections &lt;- suppressWarnings(readr::read_csv(\"rat_inspections.csv\"))\n\n\n\nAbout the Sources\nThese data sources are both government sources, making them very reliable. They are also updated daily; for this report specifically, the latest data is from November 2024.\nAdditionally, both sources provide the location, time, and other details regarding each rat sighting and inspection. These aspects will be helpful for this report’s spatial analysis and pattern recognition.\nHowever, the rat sightings data specifically is prone to under-reporting and location-based bias, due to it only containing reported sightings. This points to a lack of contextual information; however, the rat inspections data can help supplement.\nAnother thing to point out would be the effects of the COVID-19 pandemic and quarantining, where people were in lock down and stayed inside. This points to a decrease in sightings and investigations through mid 2020 and throughout 2021. However, we can assume that both were affected equally, and thus it can be assumed there won’t be any major outliers as a result."
  },
  {
    "objectID": "individual_report.html#rat-sightings",
    "href": "individual_report.html#rat-sightings",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Rat Sightings",
    "text": "Rat Sightings\nFirst, we will remove any columns that are not necessary for our analysis.\n\n\nCode\nrat_sightings &lt;- rat_sightings |&gt;\n  select(-\"Agency Name\",\n         -\"Agency\",\n         -\"Complaint Type\",\n         -\"Descriptor\",\n         -\"Location Type\",\n         -\"Incident Address\",\n         -\"Cross Street 1\",\n         -\"Cross Street 2\",\n         -\"Intersection Street 1\",\n         -\"Intersection Street 2\",\n         -\"City\",\n         -\"Address Type\",\n         -\"Landmark\",\n         -\"Facility Type\",\n         -\"Status\",\n         -\"Due Date\",\n         -\"Resolution Action Updated Date\",\n         -\"Community Board\",\n         -\"X Coordinate (State Plane)\",\n         -\"Y Coordinate (State Plane)\",\n         -\"Park Facility Name\",\n         -\"Park Borough\",\n         -\"Vehicle Type\",\n         -\"Taxi Company Borough\",\n         -\"Taxi Pick Up Location\",\n         -\"Bridge Highway Name\",\n         -\"Bridge Highway Direction\",\n         -\"Road Ramp\",\n         -\"Bridge Highway Segment\")\n\n\nSecond, we will rename some columns for clarity.\n\n\nCode\nrat_sightings &lt;- rat_sightings |&gt;\n  rename(\"ID\" = \"Unique Key\",\n         \"CREATED_DATE\" = \"Created Date\",\n         \"CLOSED_DATE\" = \"Closed Date\",\n         \"ZIP_CODE\" = \"Incident Zip\",\n         \"STREET_NAME\" = \"Street Name\",\n         \"BOROUGH\" = \"Borough\",\n         \"LATITUDE\" = \"Latitude\",\n         \"LONGITUDE\" = \"Longitude\",\n         \"LOCATION\" = \"Location\")\n\n\nThird, we will narrow down the data range, where we will look at data from 2019 to present day.\n\n\nCode\n# Reformat the dates and filter the data\nrat_sightings &lt;- rat_sightings |&gt;\n  # Convert to datetime format\n  mutate(\n    CREATED_DATE = mdy_hms(CREATED_DATE),\n    CLOSED_DATE = mdy_hms(CLOSED_DATE)) |&gt;\n  # Filter rows for dates from 2019+\n  filter(\n    CREATED_DATE &gt;= as.Date(\"2019-01-01\") |\n    CLOSED_DATE &gt;= as.Date(\"2019-01-01\")) |&gt;\n  # Reformat to MM/DD/YYYY\n  mutate(\n    CREATED_DATE = format(CREATED_DATE, \"%m/%d/%Y\"),\n    CLOSED_DATE = format(CLOSED_DATE, \"%m/%d/%Y\"))"
  },
  {
    "objectID": "individual_report.html#rat-inspections",
    "href": "individual_report.html#rat-inspections",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Rat Inspections",
    "text": "Rat Inspections\nFirst, we will remove any columns that are not necessary for our analysis.\n\n\nCode\nrat_inspections &lt;- rat_inspections |&gt;\n  select(-\"JOB_TICKET_OR_WORK_ORDER_ID\",\n         -\"JOB_PROGRESS\",\n         -\"BORO_CODE\",\n         -\"BBL\",\n         -\"BLOCK\",\n         -\"LOT\",\n         -\"HOUSE_NUMBER\",\n         -\"X_COORD\",\n         -\"Y_COORD\",\n         -\"RESULT\",\n         -\"LOCATION\",\n         -\"COMMUNITY BOARD\",\n         -\"COUNCIL DISTRICT\",\n         -\"CENSUS TRACT\",\n         -\"BIN\",\n         -\"NTA\")\n\n\nSecond, we will fix up the data in some of columns for consistency.\n\n\nCode\nrat_inspections &lt;- rat_inspections |&gt;\n  mutate(INSPECTION_TYPE = toupper(INSPECTION_TYPE),\n         BOROUGH = toupper(BOROUGH))\n\n\nThird, we will narrow down the data range, where we will look at data from 2019 to present day.\n\n\nCode\n# Reformat the dates and filter the data\nrat_inspections &lt;- rat_inspections |&gt;\n  # Convert to datetime format\n  mutate(\n    INSPECTION_DATE = mdy_hms(INSPECTION_DATE),\n    APPROVED_DATE = mdy_hms(APPROVED_DATE)) |&gt;\n  # Filter rows for dates from 2019+\n  filter(\n    INSPECTION_DATE &gt;= as.Date(\"2019-01-01\") |\n    APPROVED_DATE &gt;= as.Date(\"2019-01-01\")) |&gt;\n  # Reformat to MM/DD/YYYY\n  mutate(\n    INSPECTION_DATE = format(INSPECTION_DATE, \"%m/%d/%Y\"),\n    APPROVED_DATE = format(APPROVED_DATE, \"%m/%d/%Y\"))\n\n\nFourth, we will create a new column to combine both LATITUDE and LONGITUDE, called LOCATION, which is also found in the rat_sightings data set.\n\n\nCode\nrat_inspections &lt;- rat_inspections |&gt;\n  mutate(LOCATION = paste(\"(\", LATITUDE, \",\", \n                          LONGITUDE, \")\", \n                          sep = \"\"))"
  },
  {
    "objectID": "individual_report.html#per-borough",
    "href": "individual_report.html#per-borough",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Per Borough",
    "text": "Per Borough\nWe’re going to examine how many sightings and inspections were made per borough.\n\n\nCode\n# Sightings per borough\nsightings_per_borough &lt;- rat_sightings |&gt;\n  group_by(BOROUGH) |&gt;\n  summarize(Sightings = n(), .groups = \"drop\")\n\n# Inspections per borough\ninspections_per_borough &lt;- rat_inspections |&gt;\n  group_by(BOROUGH) |&gt;\n  summarize(Inspections = n(), .groups = \"drop\")\n\n# Put into 1 table\ncombined_rats &lt;- full_join(sightings_per_borough,\n                           inspections_per_borough,\n                           by = \"BOROUGH\") |&gt;\n  arrange(BOROUGH)\n\n# Data table\ndatatable(setNames(combined_rats,\n                   c(\"Borough\", \"Sightings\", \"Inspections\")),\n          caption = \"Table 1: Rat Sightings and Inspections per Borough\")\n\n\n\n\n\n\nWe can see that Brooklyn has most rat sightings and inspections, at 51,929 and 396,068, respectively.\nThese numbers can be a result of a few factors. Firstly, high population density, which leads to more waste production and thus an ample food source for rats. Secondly, aging infrastructure, which offer ideal nesting and breeding grounds for rats. Thirdly, as mentioned before, reporting bias.\nOn the other hand, Staten Island has the least, with 4,130 and 15,798, respectively.\nThese numbers can be a result of a few factors. Firstly, lower population density. Secondly, there’s more green space, which doesn’t support large rat populations like urban zones do. Thirdly, there are proactive community efforts for cleanliness and rodent control in public spaces."
  },
  {
    "objectID": "individual_report.html#types-of-inspections",
    "href": "individual_report.html#types-of-inspections",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Types of Inspections",
    "text": "Types of Inspections\nBefore we look into the DOH’s effect on rat sightings, we need to see what actions are being taken.\n\n\nCode\n# Count occurrences of each inspection type\ninspection_type_counts &lt;- rat_inspections |&gt;\n  count(INSPECTION_TYPE, name = \"Count\") |&gt;\n  arrange(desc(Count))\n\ndatatable(setNames(inspection_type_counts,\n                   c(\"Inspection Type\", \"Count\")),\n          caption = \"Table 4: Types of Rat Inspections\")\n\n\n\n\n\n\nThere are 3 types of inspections: initial, compliance, and bait. Initial refers to the first inspection. If a property were to fail this initial inspection, the DOH will conduct a compliance inspection, which is followed by baiting. This refers to the application of rodenticide or a monitored visit by a Health Department Pest Control Professional.\nIn this data, we can see that not all 770,669 initial inspections and 180,225 compliance inspections resulted in the need for action, with there only being 161,334 cases of baiting. For this report’s purposes, we will focus on the baiting cases.\n\nBait Inspections\nThe following code will focus on the bait inspections per year.\n\n\nCode\n# Filter and summarize the data for \"BAIT\"\nbait_per_year &lt;- rat_inspections |&gt;\n  filter(INSPECTION_TYPE == \"BAIT\") |&gt;\n  mutate(Year = year(INSPECTION_DATE)) |&gt;\n  filter(Year &gt;= 2019, Year &lt;= 2024) |&gt; \n  group_by(Year) |&gt; \n  summarize(Count = n(), \n            .groups = \"drop\") |&gt;\n  arrange(Year)\n\n# Create a data table\ndatatable(bait_per_year, \n          options = list(pageLength = 10, autoWidth = TRUE),\n          caption = \"Table 5: Bait Inspections (2019-2024)\")\n\n\n\n\n\n\nCode\n# Create a bar graph\nggplot(bait_per_year, aes(x = Year, \n                          y = Count, \n                          fill = as.factor(Year))) +\n  geom_bar(stat = \"identity\", \n           fill = \"#BA55D3\", \n           color = \"black\") +\n  labs(title = \"Figure 5: Bait Inspections (2019-2024)\",\n       x = \"Year\",\n       y = \"Count\") +\n  scale_x_continuous(breaks = seq(min(bait_per_year$Year),\n                                  max(bait_per_year$Year), by = 1)) +\n  scale_y_continuous(breaks = seq(0, max(bait_per_year$Count), \n                                  by = 5000)) +\n  geom_text(aes(label = Count), \n            vjust = -0.5, \n            color = \"black\", \n            size = 3) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nAcross all the inspections thus far, we can see a large drop in 2020 and 2021; this can be attributed to the COVID-19 pandemic, as mentioned earlier. However, actions were still made against the rat population, and thus they will still be considered."
  },
  {
    "objectID": "finalproject.html",
    "href": "finalproject.html",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "",
    "text": "Author: Thanh Dao\nLast Updated: November 20th, 2024 @11:30PM"
  },
  {
    "objectID": "finalproject.html#r-packages",
    "href": "finalproject.html#r-packages",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "R Packages",
    "text": "R Packages\nFirst, we will download the necessary R packages that will be used throughout the report.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(DT)"
  },
  {
    "objectID": "finalproject.html#data-sources",
    "href": "finalproject.html#data-sources",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Data Sources",
    "text": "Data Sources\nSecond, we will download the necessary data sources. Namely, we will be downloading the following:\n\nRat Sightings from NYC Open Data\nRodent Inspection from NYC Open Data\n\n\n\nCode\nrat_sightings &lt;- readr::read_csv(\"rat_sightings.csv\")\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nCode\nrat_inspections &lt;- readr::read_csv(\"rat_inspections.csv\")\n\n\n\nAbout the Sources\nThese data sources are both government sources, making them very reliable. They are also updated daily; for this report specifically, the latest data is from November 2024.\nAdditionally, both sources provide the location, time, and other details regarding each rat sighting and inspection. These aspects will be helpful for this report’s spatial analysis and pattern recognition.\nHowever, the rat sightings data specifically is prone to under-reporting and location-based bias, due to it only containing reported sightings. This points to a lack of contextual information; however, the rat inspections data can help supplement."
  },
  {
    "objectID": "finalproject.html#rat-sightings",
    "href": "finalproject.html#rat-sightings",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Rat Sightings",
    "text": "Rat Sightings"
  },
  {
    "objectID": "finalproject.html#rat-inspections",
    "href": "finalproject.html#rat-inspections",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Rat Inspections",
    "text": "Rat Inspections"
  },
  {
    "objectID": "mp04.html",
    "href": "mp04.html",
    "title": "Mini-Project #04",
    "section": "",
    "text": "Author: Thanh Dao\nLast Updated: December 2nd, 2024 @11:20PM"
  },
  {
    "objectID": "mp04.html#plan-1-teachers-retirement-system-trs",
    "href": "mp04.html#plan-1-teachers-retirement-system-trs",
    "title": "Mini-Project #04",
    "section": "Plan 1: Teachers Retirement System (TRS)",
    "text": "Plan 1: Teachers Retirement System (TRS)\nIn the TRS plan, CUNY will continue to pay the employee a fraction of their salary until death after they retire. The rates are as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe retirement benefit is calculated based on the final average salary of the employee, which is based on the final three years salary. The benefit will be paid out equally over 12 months, and can be calculated as follows, where N is the number of years served:\n\n\\(1.67\\% * \\text{FAS} * N\\) if \\(N \\leq 20\\)\n\\(1.75\\% * \\text{FAS} * N\\) if \\(N = 20\\)\n\\((35\\% + 2\\% * N) * \\text{FAS}\\) if \\(N \\geq 20\\)[^3]\n\nThe benefit will be increased annually by 50% of the CPI, rounded up to the nearest tenth of a percent. This inflation adjustment is done each September, and the CPI is used to aggregate the monthly CPI of the previous 12 months."
  },
  {
    "objectID": "mp04.html#plan-2-optional-retirement-plan-orp",
    "href": "mp04.html#plan-2-optional-retirement-plan-orp",
    "title": "Mini-Project #04",
    "section": "Plan 2: Optional Retirement Plan (ORP)",
    "text": "Plan 2: Optional Retirement Plan (ORP)\nThe ORP plan functions like a 401(k), where both the employee and employer contribute to a retirement account invested in mutual funds. These investments grow tax-free until retirement, with unused funds passed to heirs. It’s a defined-contribution plan, meaning contributions are fixed, but the account balance depends on market performance. At retirement, employees can withdraw funds at their discretion, typically at a 4% annual rate, while unwithdrawn funds continue earning market returns. Social Security and other savings may help cover expenses if funds are exhausted.\nThe funds available in an ORP account depends on the investments chosen; it can be assumed that OPR participants invest in Fidelity Freedom Funds, with the following asset allocation:\n\nAge 25 to Age 49:\n\n54% US Equities\n36% International Equities\n10% Bonds\n\nAge 50 to Age 59:\n\n47% US Equities\n32% International Equities\n21% Bonds\n\nAge 60 to Age 74:\n\n34% US Equities\n23% International Equities\n43% Bonds\n\nAge 75 or older:\n\n19% US Equities\n13% International Equities\n62% Bonds\n6% Short-Term Debt\n\n\nThe employee’s monthly contribution rate is as follows:\n\n$45,000 or less: 3%\n$45,001 to $55,000: 3.5%\n$55,001 to $75,000: 4.5%\n$75,001 to $100,000: 5.75%\n$100,001 or more: 6%\n\nThe employer’s contribution is fixed as follows:\n\n8% for the first seven years of employment at CUNY\n10% for all years thereafter"
  },
  {
    "objectID": "mp04.html#r-packages",
    "href": "mp04.html#r-packages",
    "title": "Mini-Project #04",
    "section": "R Packages",
    "text": "R Packages\nFirst, we will download the necessary R packages that will be used throughout the report.\n\n\nCode\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(DT)\nlibrary(httr2)\nlibrary(jsonlite)\nlibrary(reshape2)"
  },
  {
    "objectID": "mp04.html#data-sources",
    "href": "mp04.html#data-sources",
    "title": "Mini-Project #04",
    "section": "Data Sources",
    "text": "Data Sources\n\nAlphaVantage\nAlphaVantage is a commercial stock market data provider. For this report, the free subscription tier will be used. A free API key can be created through this link. Mine is stored in a txt file.\n\n\nCode\nav_key &lt;- readLines(\"/home/virgo/STA9750-2024-FALL/alphavantage_key.txt\")[1]\n\n\nNow that we have access to AlphaVantage, we’ll create a function to fetch the data.\n\n\nCode\n# Define a function to fetch data from AlphaVantage\nget_alpha_data &lt;- function(symbol, \n                           interval = \"TIME_SERIES_DAILY\", \n                           api_key) {\n  url &lt;- paste0(\"https://www.alphavantage.co/query?function=\", \n                interval,\n                \"&symbol=\", \n                symbol, \n                \"&apikey=\", api_key, \n                \"&outputsize=full&datatype=json\")\n  \n  # Send request, check response\n  response &lt;- request(url) |&gt; \n    req_perform()\n  if (response |&gt; \n      resp_status() != 200) {\n    stop(\"Failed to retrieve Alpha Vantage data. HTTP Status: \", response |&gt;\n           resp_status())}\n  \n  data &lt;- fromJSON(response |&gt; \n                     resp_body_string())\n  timeseries &lt;- data[[\"Time Series (Daily)\"]]\n  if (is.null(timeseries)) stop(\"Failed to retrieve Alpha Vantage data for symbol: \",\n                                symbol)\n  \n  df &lt;- as.data.frame(do.call(rbind, timeseries))\n  df$date &lt;- rownames(df)\n  rownames(df) &lt;- NULL\n  \n  # Data cleaning + processing\n  df &lt;- df |&gt;\n    rename(close = `4. close`) |&gt;\n    mutate(\n      date = as.Date(date),\n      close = as.numeric(close)) |&gt;\n    arrange(date)\n  \n  df &lt;- df |&gt;\n    mutate(month = format(date, \"%Y-%m\")) |&gt;\n    group_by(month) |&gt;\n    summarize(\n      monthly_return = last(close) / first(close) - 1,\n      .groups = 'drop') |&gt;\n    mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;\n    select(date, monthly_return)\n  \n  return(df)}\n\n\n\n\nFederal Reserve Economic Data (FRED)\nFRED is a repository maintained by the Federal Reserve Bank of St. Louis that is free to access. Similarly to AlphaVantage, a free API key can be created through this link. This is also stored in a txt file.\n\n\nCode\nfred_key &lt;- readLines(\"/home/virgo/STA9750-2024-FALL/fred_key.txt\")[1]\n\n\nNow that we have access to FRED, we’ll create a function to fetch the data.\n\n\nCode\nget_fred_data &lt;- function(series_id, api_key) {\n  url &lt;- paste0(\"https://api.stlouisfed.org/fred/series/observations?series_id=\", \n                series_id, \n                \"&api_key=\", \n                api_key, \n                \"&file_type=json\")\n  \n  # Send request, check response\n  response &lt;- request(url) |&gt; \n    req_perform()\n  if (response |&gt; \n      resp_status() != 200) {\n    stop(\"Failed to retrieve FRED data. HTTP Status: \", \n         response |&gt; \n           resp_status())}\n  \n  # Parse JSON response\n  data &lt;- fromJSON(response |&gt; \n                     resp_body_string())\n  if (is.null(data$observations)) stop(\"No observations found for series: \", \n                                       series_id)\n  \n  # Convert to data frame\n  df &lt;- as.data.frame(data$observations) |&gt;\n    mutate(\n      date = as.Date(date),\n      value = suppressWarnings(as.numeric(value))) |&gt;\n    filter(!is.na(value)) |&gt;\n    select(date, value)\n  \n  return(df)}\n\n\n\n\nHistorical Data\nFor this analysis, we need historical data covering the following:\n\nWage growth rate from FRED (Average Hourly Earnings Growth)\nInflation from FRED (Consumer Price Index)\nUS Equity Market total returns from AlphaVantage (SPY)\nInternational Equity Market total returns from AlphaVantage (ACWI)\nBond market total returns from FRED (10-Year Treasury Yield)\nShort-term debt returns from FRED (2-Year Treasury Yield)\n\nWe will identify and download the data series of each of the above inputs, from both AlphaVantage and FRED.\n\n\nCode\n# Wage growth rate\nwage_growth &lt;- get_fred_data(\"CES0500000003\", \n                             fred_key) |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;\n  group_by(month) |&gt;\n  summarize(wage_growth_rate = last(value), \n            .groups = 'drop') |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;\n  select(date, wage_growth_rate)\n\n# Inflation\ninflation &lt;- get_fred_data(\"CPIAUCSL\", \n                           fred_key) |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;\n  group_by(month) |&gt;\n  summarize(inflation_rate = last(value), \n            .groups = 'drop') |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;\n  select(date, inflation_rate)\n\n# US Equity Market total returns\nus_equity_market &lt;- get_alpha_data(\"SPY\", \"TIME_SERIES_DAILY\", \n                                 av_key) |&gt;\n  rename(us_equity_return = monthly_return)\n\n# International Equity Market total returns\nintl_equity_market &lt;- get_alpha_data(\"ACWI\", \n                                   \"TIME_SERIES_DAILY\", \n                                   av_key) |&gt;\n  rename(intl_equity_return = monthly_return)\n\n# Bond market total returns\nbond_market &lt;- get_fred_data(\"GS10\", \n                           fred_key) |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;\n  group_by(month) |&gt;\n  summarize(bond_return = last(value), \n            .groups = 'drop') |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;\n  select(date, bond_return)\n\n# Short-term debt returns\nshort_term_debt &lt;- get_fred_data(\"DGS2\", fred_key) |&gt;\n  mutate(month = format(date, \"%Y-%m\")) |&gt;\n  group_by(month) |&gt;\n  summarize(short_term_rate = last(value), \n            .groups = 'drop') |&gt;\n  mutate(date = as.Date(paste0(month, \"-01\"))) |&gt;\n  select(date, short_term_rate)\n\n\nFor ease, we’ll merge all of the data into one.\n\n\nCode\nmerged_data &lt;- list(wage_growth,\n                 inflation,\n                 us_equity_market,\n                 intl_equity_market,\n                 bond_market,\n                 short_term_debt) |&gt;\n  reduce(full_join, by = \"date\") |&gt;\n  arrange(date) |&gt;\n  drop_na()"
  },
  {
    "objectID": "individual_report.html#basic-nyc-map",
    "href": "individual_report.html#basic-nyc-map",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Basic NYC Map",
    "text": "Basic NYC Map\nThe following code will obtain the .shp files to create a map for NYC.\n\n\nCode\n# Get the NYC borough boundaries\nif(!file.exists(\"nyc_borough_boundaries.zip\")){\n  download.file(\"https://data.cityofnewyork.us/api/geospatial/tqmj-j8zm?method=export&format=Shapefile\", \n                destfile=\"nyc_borough_boundaries.zip\")\n}\n\n# Define the function to read .shp file from a zip archive\nread_shp_from_zip &lt;- function(zip_file) {\n  # Create a temporary directory\n  td &lt;- tempdir(); \n  # Extract the contents of the zip file\n  zip_contents &lt;- unzip(\"nyc_borough_boundaries.zip\", \n                        exdir = td)\n  # Identify the .shp file among the extracted contents\n  fname_shp &lt;- zip_contents[grepl(\"shp$\", zip_contents)]\n  # Read the shapefile into R using st_read\n  nyc_sf &lt;- st_read(fname_shp)\n  return(nyc_sf)\n}\n\nnyc_sf &lt;- read_shp_from_zip(\"nyc_borough_boundaries.zip\")\n\n\nReading layer `geo_export_992fdf87-1fb2-4c6c-b53c-f66dbdaa04a5' from data source `/tmp/Rtmp9dLpot/geo_export_992fdf87-1fb2-4c6c-b53c-f66dbdaa04a5.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -74.25559 ymin: 40.49613 xmax: -73.70001 ymax: 40.91553\nGeodetic CRS:  WGS84(DD)"
  },
  {
    "objectID": "individual_report.html#preliminary-maps",
    "href": "individual_report.html#preliminary-maps",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Preliminary Maps",
    "text": "Preliminary Maps\nThe following choropleth maps will display similar results to what was found in the previous table.\n\nNYC Map of Rat Sightings\nThe following code will create a choropleth map using the rat_sightings data.\n\n\nCode\n# Ensure column names match\nnyc_sf &lt;- nyc_sf |&gt;\n  rename(BOROUGH = boro_name)\n\nnyc_sf &lt;- nyc_sf |&gt;\n  mutate(BOROUGH = toupper(BOROUGH))\n\n# Join spatial data with previously made sightings / inspection data\nnyc_sf_rats &lt;- nyc_sf |&gt;\n  left_join(combined_rats, by = \"BOROUGH\")\n\n# Create map with fill based on sightings\nggplot(nyc_sf_rats, \n       aes(geometry = geometry, fill = Sightings)) + \n  geom_sf() +\n  scale_fill_gradient(low = \"#E3CEF6\", \n                      high = \"#4B0082\", \n                      na.value = \"gray\") +  \n  theme_minimal() + \n  labs(title = \"Figure 1: Rat Sightings per Borough in NYC (2019-2024)\",\n       fill = \"Sightings\",)\n\n\n\n\n\n\n\n\n\nWe can see that Brooklyn has the most sightings, and Staten Island has the least; however, this can also be connected to the population sizes.\n\n\nNYC Map of Rat Inspections\nNow, we’re going to create a choropleth map using the rat_inspections data.\n\n\nCode\nggplot(nyc_sf_rats, \n       aes(geometry = geometry, fill = Sightings)) + \n  geom_sf() +\n  scale_fill_gradient(low = \"#E3CEF6\", \n                      high = \"#4B0082\", \n                      na.value = \"gray\") +  \n  theme_minimal() + \n  labs(title = \"Figure 2: Rat Inspections per Borough in NYC (2019-2024)\",\n       fill = \"Inspections\")\n\n\n\n\n\n\n\n\n\nSimilarly to rat sightings, we can see that Brooklyn has the most inspections, and Staten Island has the least."
  },
  {
    "objectID": "individual_report.html#rat-sightings-1",
    "href": "individual_report.html#rat-sightings-1",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Rat Sightings",
    "text": "Rat Sightings\nThe following code will display the difference in rat sightings across the five years in both a table and through a bar chart.\n\n\nCode\n# Ensure CREATED_DATE is in date format\nrat_sightings &lt;- rat_sightings |&gt;\n  mutate(CREATED_DATE = as.Date(CREATED_DATE, \n                                format = \"%m/%d/%Y\"))\n\n# Group by year, filter for 2019 onward, and summarize inspections\nsightings_per_year &lt;- rat_sightings |&gt;\n  mutate(Year = year(CREATED_DATE)) |&gt;  # Extract year\n  filter(Year &gt;= 2019) |&gt;\n  group_by(Year) |&gt; \n  summarize(Sightings = n(), \n            .groups = \"drop\") |&gt;\n  arrange(Year)\n\n# Show the difference in sightings between years\nsightings_per_year &lt;- sightings_per_year |&gt;\n  mutate(Difference = Sightings - lag(Sightings, \n                                      order_by = Year))\n\n# Display as a data table\ndatatable(sightings_per_year, \n          caption = \"Number of Rat Sightings per Year\", \n          options = list(pageLength = 10, autoWidth = TRUE))\n\n\n\n\n\n\nCode\n# Display as a graph\nggplot(sightings_per_year, \n       aes(x = Year, \n           y = Sightings)) +\n  geom_bar(stat = \"identity\", fill = \"#BA55D3\", color = \"black\") +\n  labs(title = \"NYC Rat Sightings per Year (2019+)\",\n       x = \"Year\",\n       y = \"# of Sightings\") +\n  scale_x_continuous(breaks = seq(min(sightings_per_year$Year), \n                                  max(sightings_per_year$Year), \n                                  by = 1)) +\n  scale_y_continuous(breaks = seq(0, max(sightings_per_year$Sightings),\n                                  by = 5000)) +\n  geom_text(aes(label = Sightings), \n            vjust = -0.5, \n            color = \"black\",\n            size = 3) +\n  theme_linedraw()"
  },
  {
    "objectID": "individual_report.html#rat-inspections-1",
    "href": "individual_report.html#rat-inspections-1",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Rat Inspections",
    "text": "Rat Inspections\n\n\nCode\n# Ensure INSPECTION_DATE is in date format\nrat_inspections &lt;- rat_inspections |&gt;\n  mutate(INSPECTION_DATE = as.Date(INSPECTION_DATE, \n                                   format = \"%m/%d/%Y\"))\n\n# Group by year, filter for 2019 onward, and summarize inspections\ninspections_per_year &lt;- rat_inspections |&gt;\n  mutate(Year = year(INSPECTION_DATE)) |&gt; \n  filter(Year &gt;= 2019, Year &lt;= 2024) |&gt;  \n  group_by(Year) |&gt; \n  summarize(Inspections = n(), \n            .groups = \"drop\") |&gt;\n  arrange(Year) \n\n# Show the difference in inspections between years\ninspections_per_year &lt;- inspections_per_year |&gt;\n  mutate(Difference = Inspections - lag(Inspections, order_by = Year))\n\n# Display as a data table\ndatatable(inspections_per_year, \n          caption = \"Number of Rat Inspections per Year\", \n          options = list(pageLength = 10, autoWidth = TRUE))\n\n\n\n\n\n\nCode\n# Display as a graph\nggplot(inspections_per_year, \n       aes(x = Year, y = Inspections)) +\n  geom_bar(stat = \"identity\", \n           fill = \"#BA55D3\", \n           color = \"black\") +\n  labs(title = \"Rat Inspections per Year (2019+)\",\n       x = \"Year\",\n       y = \"# of Inspections\") +\n  scale_x_continuous(breaks = seq(min(inspections_per_year$Year), \n                                  max(inspections_per_year$Year), \n                                  by = 1)) +\n  scale_y_continuous(breaks = seq(0,\n                                  max(inspections_per_year$Inspections),\n                                  by = 20000)) +\n  geom_text(aes(label = Inspections), \n            vjust = -0.5, color = \"black\", size = 3) +\n  theme_minimal()"
  },
  {
    "objectID": "individual_report.html#over-time",
    "href": "individual_report.html#over-time",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Over Time",
    "text": "Over Time\nWe will now examine the change in numbers across the five years; thus, the following calculations will be made.\n\nRat Sightings\nThe following code will display the difference in rat sightings across the five years in both a table and through a bar chart.\n\n\nCode\n# Ensure CREATED_DATE is in date format\nrat_sightings &lt;- rat_sightings |&gt;\n  mutate(CREATED_DATE = as.Date(CREATED_DATE, \n                                format = \"%m/%d/%Y\"))\n\n# Group by year, filter for 2019 onward, and summarize inspections\nsightings_per_year &lt;- rat_sightings |&gt;\n  mutate(Year = year(CREATED_DATE)) |&gt;  # Extract year\n  filter(Year &gt;= 2019) |&gt;\n  group_by(Year) |&gt; \n  summarize(Sightings = n(), \n            .groups = \"drop\") |&gt;\n  arrange(Year)\n\n# Show the difference in sightings between years\nsightings_per_year &lt;- sightings_per_year |&gt;\n  mutate(Difference = Sightings - lag(Sightings, \n                                      order_by = Year))\n\n# Display as a data table\ndatatable(sightings_per_year, \n          caption = \"Table 2: Number of Rat Sightings per Year\", \n          options = list(pageLength = 10, autoWidth = TRUE))\n\n\n\n\n\n\nCode\n# Display as a graph\nggplot(sightings_per_year, \n       aes(x = Year, \n           y = Sightings)) +\n  geom_bar(stat = \"identity\", fill = \"#BA55D3\", color = \"black\") +\n  labs(title = \"Figure 3: NYC Rat Sightings per Year (2019-2024)\",\n       x = \"Year\",\n       y = \"# of Sightings\") +\n  scale_x_continuous(breaks = seq(min(sightings_per_year$Year), \n                                  max(sightings_per_year$Year), \n                                  by = 1)) +\n  scale_y_continuous(breaks = seq(0, max(sightings_per_year$Sightings),\n                                  by = 5000)) +\n  geom_text(aes(label = Sightings), \n            vjust = -0.5, \n            color = \"black\",\n            size = 3) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n\nRat Inspections\nThe following code will display the difference in rat inspections across the five years in both a table and through a bar chart.\n\n\nCode\n# Ensure INSPECTION_DATE is in date format\nrat_inspections &lt;- rat_inspections |&gt;\n  mutate(INSPECTION_DATE = as.Date(INSPECTION_DATE, \n                                   format = \"%m/%d/%Y\"))\n\n# Group by year, filter for 2019 onward, and summarize inspections\ninspections_per_year &lt;- rat_inspections |&gt;\n  mutate(Year = year(INSPECTION_DATE)) |&gt; \n  filter(Year &gt;= 2019, Year &lt;= 2024) |&gt;  \n  group_by(Year) |&gt; \n  summarize(Inspections = n(), \n            .groups = \"drop\") |&gt;\n  arrange(Year) \n\n# Show the difference in inspections between years\ninspections_per_year &lt;- inspections_per_year |&gt;\n  mutate(Difference = Inspections - lag(Inspections, order_by = Year))\n\n# Display as a data table\ndatatable(inspections_per_year, \n          caption = \"Table 3: Number of Rat Inspections per Year\", \n          options = list(pageLength = 10, autoWidth = TRUE))\n\n\n\n\n\n\nCode\n# Display as a graph\nggplot(inspections_per_year, \n       aes(x = Year, y = Inspections)) +\n  geom_bar(stat = \"identity\", \n           fill = \"#BA55D3\", \n           color = \"black\") +\n  labs(title = \"Figure 4: NYC Rat Inspections per Year (2019-2024)\",\n       x = \"Year\",\n       y = \"# of Inspections\") +\n  scale_x_continuous(breaks = seq(min(inspections_per_year$Year), \n                                  max(inspections_per_year$Year), \n                                  by = 1)) +\n  scale_y_continuous(breaks = seq(0,\n                                  max(inspections_per_year$Inspections),\n                                  by = 20000)) +\n  geom_text(aes(label = Inspections), \n            vjust = -0.5, color = \"black\", size = 3) +\n  theme_linedraw()"
  },
  {
    "objectID": "individual_report.html#comparing-sightings-to-bait-inspections",
    "href": "individual_report.html#comparing-sightings-to-bait-inspections",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Comparing Sightings to Bait Inspections",
    "text": "Comparing Sightings to Bait Inspections\nWe will now compare the sightings data to the bait data; specifically, we will examine the number of sightings per borough before and after the bait was applied.\n\nOverall\nBefore we do a “before and after” application of bait, We will create a correlation map as is.\n\n\nCode\n# Merge datasets on \"Year\"\nmerged_data &lt;- merge(sightings_per_year,\n                     bait_per_year,\n                     by = \"Year\")\n\n# Select the relevant columns for correlation\ncorr_data &lt;- merged_data[, c(\"Sightings\", \"Count\")]\n\n# Compute correlation matrix\ncorrelation_matrix &lt;- cor(corr_data, use = \"complete.obs\")\n\n# Generate the correlation plot\ncorrplot(\n  correlation_matrix, \n  method = \"circle\", \n  type = \"full\", \n  col = colorRampPalette(c(\"plum\", \"white\", \"purple\"))(200),\n  addCoef.col = \"black\", \n  number.cex = 0.7, \n  tl.col = \"black\", \n  tl.cex = 0.8, \n  cl.cex = 0.8,\n  title = \"Figure 6: Correlation between Sightings and Bait Count\", \n  mar = c(0, 0, 2, 0))\n\n\nWarning in ind1:ind2: numerical expression has 2 elements: only the first used\n\n\n\n\n\n\n\n\n\nBased on the correlation coefficient of 0.34, there is some degree of positive association, albeit somewhat weak, between the rat sightings and bait count. As rat sightings increased, bait count also increased, but the relationship isn’t very strong or consistent.\nWe’ll also create a bar chart that combines Figures 3 and 5, to give a better perspective.\n\n\nCode\n# Add a new column for the variable type\nsightings &lt;- sightings_per_year |&gt;\n  mutate(Type = \"Sightings\", Value = Sightings)\n\nbait &lt;- bait_per_year |&gt;\n  mutate(Type = \"Bait Count\", Value = Count)\n\n# Combine the datasets\ncombined_data &lt;- bind_rows(sightings[, c(\"Year\", \"Type\", \"Value\")], \n                           bait[, c(\"Year\", \"Type\", \"Value\")])\n\n# Create combined bar chart\nggplot(combined_data, aes(x = as.factor(Year), \n                          y = Value, \n                          fill = Type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(\n    title = \"Figure 7: NYC Rat Sightings and Bait Inspections (2019-2024)\",\n    x = \"Year\",\n    y = \"Count\",\n    fill = \"Type\") +\n  scale_fill_manual(values = c(\"Sightings\" = \"#BA55D3\", \n                               \"Bait Count\" = \"#6495ED\")) +\n  scale_y_continuous(breaks = seq(0, max(combined_data$Value), \n                                  by = 5000)) +\n  geom_text(\n    aes(label = Value), \n    position = position_dodge(width = 0.9), \n    vjust = -0.5, \n    size = 3) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nOverall, there appears to be a positive relationship between bait inspections and rat sightings. However, the intensity of bait inspections doesn’t always lead to a proportional change in sightings. For instance, 2024 had higher bait inspections than 2021 but lower sightings. This agrees with the correlation coefficient of 0.34 determined earlier.\n\n\nYearly\nWe will now examine this through a “cause and effect” lens, where we examine the correlation between bait inspections from one year and rat sightings in the following year.\n\n\nCode\n# Lag sightings data by 1 year\nsightings_lagged &lt;- sightings_per_year |&gt;\n  mutate(Year = Year - 1) |&gt;\n  rename(Lagged_Sightings = Sightings)\n\n# Merge with bait_per_year on Year\nyearly_cause_effect &lt;- bait_per_year |&gt;\n  inner_join(sightings_lagged, by = \"Year\")\n\n# Create a correlation matrix for Bait Count and Lagged Sightings\nyearly_cause_effect_corr &lt;- yearly_cause_effect[, c(\"Count\", \"Lagged_Sightings\")]\n\n# Compute correlation matrix\nyearly_correlation_matrix &lt;- cor(yearly_cause_effect_corr,\n                                 use = \"complete.obs\")\n\n# Generate correlation plot\ncorrelation_plot &lt;- corrplot(\n  yearly_correlation_matrix, \n  method = \"circle\", \n  type = \"full\", \n  col = colorRampPalette(c(\"plum\", \"white\", \"purple\"))(200), \n  addCoef.col = \"black\", \n  number.cex = 0.7, \n  tl.col = \"black\", \n  tl.cex = 0.8, \n  cl.cex = 0.8, \n  title = \"Figure 8: Correlation of Bait Inspections and Next-Year Sightings\", \n  mar = c(0, 0, 2, 0)\n)\n\n\nWarning in ind1:ind2: numerical expression has 2 elements: only the first used\n\n\n\n\n\n\n\n\n\nBased on the correlation coefficient of -0.53, we can determine that, in general, as the number of bait inspections increases in one year, the number of rat sightings tends to decrease in the following year, but with moderate strength. This means that higher bait inspections could have a somewhat beneficial impact on reducing sightings, but the relationship is not extremely strong.\nThis also goes against what was previously determined with the overall correlation.\n\n\nMonthly\nWe will also look into this on a monthly basis."
  },
  {
    "objectID": "mp04.html#correlations",
    "href": "mp04.html#correlations",
    "title": "Mini-Project #04",
    "section": "Correlations",
    "text": "Correlations\nTo examine the correlation between these aspects, we’ll create a table to display it, and a correlation heat map.\n\n\nCode\n# Exclude the date column\ncor_data &lt;- merged_data |&gt;\n  select(-date)\n\n# Compute correlation matrix\ncor_matrix &lt;- cor(cor_data, use = \"complete.obs\")\n\n# Melt correlation matrix for visualization\ncor_melt &lt;- melt(cor_matrix)\n\n# Data table\n## Remove self-correlations and mirrored values\ncor_table &lt;- cor_melt |&gt;\n  filter(Var1 != Var2) |&gt; # Remove self-correlations\n  mutate(pair = pmap_chr(list(Var1, Var2), \n                         ~paste(sort(c(.x, .y)), collapse = \" - \"))) |&gt;\n  distinct(pair, .keep_all = TRUE) |&gt; # Remove mirrored pairs\n  select(-pair)\n\n## Descending order + round\ncor_table &lt;- cor_table |&gt;\n  arrange(desc(value)) |&gt;\n  mutate(value = round(value, 4))\n\ndatatable(setNames(cor_table,\n                   c(\"Variable 1\", \"Variable 2\", \"Correlation Coefficient\")),\n          caption = \"Variable Correlations\")\n\n\n\n\n\n\nCode\n# Plot the heat map\nggplot(cor_melt, aes(Var1, Var2, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"white\", \n                       mid = \"palegreen\", \n                       high = \"darkgreen\", \n                       midpoint = 0) +\n  theme_light() +\n  labs(title = \"Correlation Heatmap\", \n       x = \"Variables\", \n       y = \"Variables\", \n       fill = \"Correlation\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nIn this heat map, the darker green indicates a higher correlation, and the lighter green / white indicates a lower correlation.\nSpecifically, we can see the highest correlation is between inflation rate and wage growth rate, with a correlation coefficient of 0.9892.\nThe following graph will better highlight this.\n\n\nCode\nggplot(merged_data, aes(x = date)) +\n  geom_line(aes(y = inflation_rate, \n                color = \"Inflation Rate\")) +\n  geom_line(aes(y = wage_growth_rate, \n                color = \"Wage Growth Rate\")) +\n  labs(title = \"Inflation Rate and Wage Growth Rate Over Time\",\n       x = \"Date\",\n       y = \"Rate\",\n       color = \"Variable\") +\n  theme_linedraw() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nThis means that as the inflation rate increases, the wage growth rate also tends to increase, and the two variables move in nearly perfect alignment. In an economic context, wage growth and inflation are often strongly linked because higher inflation typically pressures employers to raise wages to help workers keep up with rising living costs. Conversely, strong wage growth can contribute to higher inflation as consumers have more income to spend, increasing demand for goods and services.\nThe lowest correlation is between Bond market total returns and US Equity Market total returns, with a correlation coefficient of -0.0862.\nThe following graph will better highlight this.\n\n\nCode\nggplot(merged_data, aes(x = date)) +\n  geom_line(aes(y = us_equity_return, \n                color = \"US Equity Market Returns\")) +\n  geom_line(aes(y = bond_return, \n                color = \"Bond Market Returns\")) +\n  labs(title = \"US Equity Market Returns and Bond Market Returns Over Time\",\n       x = \"Date\",\n       y = \"Returns\",\n       color = \"Variable\") +\n  theme_linedraw() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = \"top\")\n\n\n\n\n\n\n\n\n\nThis means changes in bond market returns provide little to no information about changes in U.S. Equity Market returns. In an economic context, bonds and stocks are often considered different asset classes with unique risk and return profiles. While they can occasionally move inversely during market downturns (as investors shift to safer assets), the weak correlation here suggests no consistent relationship over the time period analyzed. This lack of correlation aligns with the idea of diversification in investment portfolios, where combining stocks and bonds can help reduce overall risk."
  },
  {
    "objectID": "mp04.html#long-term-averages",
    "href": "mp04.html#long-term-averages",
    "title": "Mini-Project #04",
    "section": "Long-Term Averages",
    "text": "Long-Term Averages"
  },
  {
    "objectID": "mp04.html#trs",
    "href": "mp04.html#trs",
    "title": "Mini-Project #04",
    "section": "TRS",
    "text": "TRS\n\n\nCode\n# Calculate TRS\ntrs_income &lt;- calculate_trs(\n  starting_salary = starting_salary,\n  wage_growth = wage_growth,\n  inflation = inflation,\n  years_worked = working_years)/100\n\ncat(\"TRS First Month Retirement Income: $\", round(trs_income, 2), \"\\n\")\n\n\nTRS First Month Retirement Income: $ 2557.31"
  },
  {
    "objectID": "mp04.html#orp",
    "href": "mp04.html#orp",
    "title": "Mini-Project #04",
    "section": "ORP",
    "text": "ORP\n\n\nCode\n# Calculate ORP\norp_income &lt;- calculate_orp(\n  starting_salary = starting_salary,\n  wage_growth = wage_growth, \n  us_equity_market = us_equity_market, \n  bond_market = bond_market, \n  years_worked = working_years)\n\ncat(\"ORP First Month Retirement Income: $\", round(orp_income, 2), \"\\n\")\n\n\nORP First Month Retirement Income: $ 2463.79"
  },
  {
    "objectID": "mp04.html#first-month-comparison-conclusion",
    "href": "mp04.html#first-month-comparison-conclusion",
    "title": "Mini-Project #04",
    "section": "First Month Comparison Conclusion",
    "text": "First Month Comparison Conclusion\nBased on this information, we can determine that the TRS first month retirement income is more beneficial than that of the ORP, with the TRS’s income being calculated at $2,557.31, based on the last 3-year-average salary and fixed formula adjustments. On the other hand, the ORP’s income was calculated at $2,463.79, based on the investment account balance and a 4$ withdrawal rate."
  },
  {
    "objectID": "mp04.html#long-term-summary-statistics",
    "href": "mp04.html#long-term-summary-statistics",
    "title": "Mini-Project #04",
    "section": "Long-Term Summary Statistics",
    "text": "Long-Term Summary Statistics\nWe will now calculate the long-term monthly averages for each variable.\n\n\nCode\n# Calculate the long-term means\nlong_term_means &lt;- merged_data |&gt;\n  select(-date) |&gt;\n  summarize_all(~ round(mean(.x, na.rm = TRUE), 4))\n\n# Calculate the long-term SD\nlong_term_sd &lt;- merged_data |&gt;\n  select(-date) |&gt;\n  summarize_all(~ round(sd(.x, na.rm = TRUE), 4))\n\n# Mean data table\ndatatable(setNames(long_term_means,\n                   c(\"Wage Growth\",\n                     \"Inflation Rate\",\n                     \"US Equity Market Total Returns\",\n                     \"International Equity Market Total Returns\",\n                     \"Bond Market Returns\",\n                     \"Short-Term Debt Returns\")),\n          caption = \"Long-Term Means of Variables\")\n\n\n\n\n\n\nCode\n# SD data table\ndatatable(setNames(long_term_sd,\n                   c(\"Wage Growth\",\n                     \"Inflation Rate\",\n                     \"US Equity Market Total Returns\",\n                     \"International Equity Market Total Returns\",\n                     \"Bond Market Returns\",\n                     \"Short-Term Debt Returns\")),\n          caption = \"Long-Term Standard Deviations of Variables\")\n\n\n\n\n\n\n\nWage Growth: The average is 26.6905, with a standard deviation of 3.9729, pointing to stable long-term wage growth and relatively low volatility.\nInflation Rate: The average is 249.212, with a standard deviation of 28.8624, pointing to a history of substantial volatility. This rate is significantly higher than the other variables, due to its identity as an index.\nUS and International Equity Market Total Returns: The US average and standard deviation are 0.0072 and 0.0481, respectively. The international average and standard deviation are 0.0042 and 0.051, respectively. The standard deviations are fairly similar, pointing to similar levels of volatility.\nBond Market and Short-Term Debt Returns: The average bond market return is 2.5848, which is higher than that of short-term debt returns of 1.4406. However, short-term debt returns have a higher volatility due to the standard deviation of 1.4122, compared to that of the bond market return with 0.9244."
  },
  {
    "objectID": "mp04.html#specific-analysis",
    "href": "mp04.html#specific-analysis",
    "title": "Mini-Project #04",
    "section": "Specific Analysis",
    "text": "Specific Analysis\n\n\nCode\n# What is the probability that an ORP employee exhausts their savings before death?\norp_depletion_probability &lt;- simulation_results %&gt;%\n  filter(orp_balance == 0) %&gt;%\n  summarize(prob = n_distinct(simulation_id) / num_simulations) %&gt;%\n  pull(prob)\n\ncat(\"Probability of ORP funds depletion: \", round(orp_depletion_probability * 100, 2), \"%\\n\")\n\n\nProbability of ORP funds depletion:  0 %\n\n\nWith a 4% withdrawal rate, the probability that the ORP employees exhaust their savings before death is 0%.\n\n\nCode\n# What is the probability that an ORP employee has a higher monthly income in retirement than a TRS employee?\norp_better_than_trs_probability &lt;- simulation_results %&gt;%\n  group_by(simulation_id) %&gt;%\n  summarize(orp_better = mean(orp_income &gt; trs_income)) %&gt;%\n  summarize(prob = mean(orp_better &gt; 0.5)) %&gt;%\n  pull(prob) \n\ncat(\"Probability ORP income exceeds TRS income: \", round(orp_better_than_trs_probability * 100, 2), \"%\\n\")\n\n\nProbability ORP income exceeds TRS income:  2.5 %\n\n\nThe probability that an ORP employee were to earn a higher monthly income than a TRS employee is 2.5%, pointing to the TRS’ consistency with providing a more stable and predictable income, compared to the ORP’s potential for higher returns in some cases."
  },
  {
    "objectID": "individual_report.html#comparing-sightings-to-bait-inspections---time",
    "href": "individual_report.html#comparing-sightings-to-bait-inspections---time",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Comparing Sightings to Bait Inspections - Time",
    "text": "Comparing Sightings to Bait Inspections - Time\nWe will now compare the sightings data to the bait data; specifically, we will examine the number of sightings per borough before and after the bait was applied through the lens of time.\n\nOverall\nBefore we do a “cause-and-effect” application of bait, We will create a correlation map as is.\n\n\nCode\n# Merge datasets on \"Year\"\nmerged_data &lt;- merge(sightings_per_year,\n                     bait_per_year,\n                     by = \"Year\")\n\n# Select the relevant columns for correlation\ncorr_data &lt;- merged_data[, c(\"Sightings\", \"Count\")]\n\n# Compute correlation matrix\ncorrelation_matrix &lt;- cor(corr_data, use = \"complete.obs\")\n\n# Generate the correlation plot\nsuppressWarnings(corrplot(\n  correlation_matrix, \n  method = \"circle\", \n  type = \"full\", \n  col = colorRampPalette(c(\"plum\", \"white\", \"purple\"))(200),\n  addCoef.col = \"black\", \n  number.cex = 0.7, \n  tl.col = \"black\", \n  tl.cex = 0.8, \n  cl.cex = 0.8,\n  title = \"Figure 6: Correlation between Sightings and Bait Count\", \n  mar = c(0, 0, 2, 0)))\n\n\n\n\n\n\n\n\n\nBased on the correlation coefficient of 0.34, there is some degree of positive association, albeit somewhat weak, between the rat sightings and bait count. As rat sightings increased, bait count also increased, but the relationship isn’t very strong or consistent.\nWe’ll also create a bar chart that combines Figures 3 and 5, to give a better perspective.\n\n\nCode\n# Add a new column for the variable type\nsightings &lt;- sightings_per_year |&gt;\n  mutate(Type = \"Sightings\", Value = Sightings)\n\nbait &lt;- bait_per_year |&gt;\n  mutate(Type = \"Bait Count\", Value = Count)\n\n# Combine the datasets\ncombined_data &lt;- bind_rows(sightings[, c(\"Year\", \"Type\", \"Value\")], \n                           bait[, c(\"Year\", \"Type\", \"Value\")])\n\n# Create combined bar chart\nggplot(combined_data, aes(x = as.factor(Year), \n                          y = Value, \n                          fill = Type)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(\n    title = \"Figure 7: NYC Rat Sightings and Bait Inspections (2019-2024)\",\n    x = \"Year\",\n    y = \"Count\",\n    fill = \"Type\") +\n  scale_fill_manual(values = c(\"Sightings\" = \"#BA55D3\", \n                               \"Bait Count\" = \"#6495ED\")) +\n  scale_y_continuous(breaks = seq(0, max(combined_data$Value), \n                                  by = 5000)) +\n  geom_text(\n    aes(label = Value), \n    position = position_dodge(width = 0.9), \n    vjust = -0.5, \n    size = 3) +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nOverall, there appears to be a positive relationship between bait inspections and rat sightings. However, the intensity of bait inspections doesn’t always lead to a proportional change in sightings. For instance, 2024 had higher bait inspections than 2021 but lower sightings. This agrees with the correlation coefficient of 0.34 determined earlier.\n\n\nYearly\nWe will now examine this through a “cause-and-effect” lens, where we examine the correlation between bait inspections from one year and rat sightings in the following year.\n\n\nCode\n# Lag sightings data by 1 year\nsightings_lagged &lt;- sightings_per_year |&gt;\n  mutate(Year = Year - 1) |&gt;\n  rename(Lagged_Sightings = Sightings)\n\n# Merge with bait_per_year on Year\nyearly_cause_effect &lt;- bait_per_year |&gt;\n  inner_join(sightings_lagged, by = \"Year\")\n\n# Create a correlation matrix for Bait Count and Lagged Sightings\nyearly_cause_effect_corr &lt;- yearly_cause_effect[, c(\"Count\", \"Lagged_Sightings\")]\n\n# Compute correlation matrix\nyearly_correlation_matrix &lt;- cor(yearly_cause_effect_corr,\n                                 use = \"complete.obs\")\n\n# Generate correlation plot\nsuppressWarnings(correlation_plot &lt;- corrplot(\n  yearly_correlation_matrix, \n  method = \"circle\", \n  type = \"full\", \n  col = colorRampPalette(c(\"plum\", \"white\", \"purple\"))(200), \n  addCoef.col = \"black\", \n  number.cex = 0.7, \n  tl.col = \"black\", \n  tl.cex = 0.8, \n  cl.cex = 0.8, \n  title = \"Figure 8: Correlation of Bait Inspections and Next-Year Sightings\", \n  mar = c(0, 0, 2, 0)))\n\n\n\n\n\n\n\n\n\nBased on the correlation coefficient of -0.53, we can determine that, in general, as the number of bait inspections increases in one year, the number of rat sightings tends to decrease in the following year, but with moderate strength. This means that higher bait inspections could have a somewhat beneficial impact on reducing sightings, and the relationship is moderately strong.\nThis also goes against what was previously determined with the overall correlation.\n\n\nMonthly\nWe will also look into this on a monthly basis.\n\n\nCode\n# Extract MONTH from rat_sightings and place in new column\nrat_sightings$MONTH &lt;- month(ymd(rat_sightings$CREATED_DATE))\n\n# Do sightings per month\nsightings_per_month &lt;- rat_sightings[, c(\"CREATED_DATE\", \"MONTH\")]\n\nsightings_per_month &lt;- sightings_per_month |&gt;\n  mutate(YEAR = format(as.Date(CREATED_DATE), \"%Y\")) |&gt; # Extract year\n  filter(YEAR &gt;= 2019 & YEAR &lt;= 2024) |&gt;                # Filter data from 2019 to 2024\n  group_by(YEAR, MONTH) |&gt;  # Group by year and month\n  summarise(NUM_SIGHTINGS = n(), \n            .groups = \"drop\")  # # of sightings per group\n\n# Extract MONTH from rat_inspections and place in new column\nrat_inspections$MONTH &lt;- month(ymd(rat_inspections$INSPECTION_DATE))\n\n# Do bait per month\nbait_per_month &lt;- rat_inspections[, c(\"INSPECTION_TYPE\", \"INSPECTION_DATE\", \"MONTH\")]\nbait_per_month &lt;- bait_per_month[bait_per_month$INSPECTION_TYPE == \"BAIT\", ]\n\nbait_per_month &lt;- bait_per_month |&gt;\n  mutate(YEAR = format(as.Date(INSPECTION_DATE), \"%Y\")) |&gt;  # Extract year\n  filter(YEAR &gt;= 2019 & YEAR &lt;= 2024) |&gt;                    # Filter data from 2019 to 2024\n  group_by(YEAR, MONTH) |&gt;  # Group by year and month\n  summarise(NUM_BAITS = n(), \n            .groups = \"drop\")  # # of bait inspections per group\n\n# Lag sightings data by 1 month\nsightings_lagged_month &lt;- sightings_per_month |&gt;\n  mutate(Month = as.integer(MONTH) + 1) |&gt;\n  rename(Lagged_Sightings = NUM_SIGHTINGS)\n\n# Ensure the \"Month\" rolls over correctly (From December to January)\nsightings_lagged_month$Month[sightings_lagged_month$Month == 13] &lt;- 1\n\n# Merge with bait_per_month on YEAR and MONTH\nmonthly_cause_effect &lt;- bait_per_month |&gt;\n  inner_join(sightings_lagged_month, by = c(\"YEAR\", \"MONTH\"))\n\n# Create a correlation matrix for Bait Count and Lagged Sightings\nmonthly_cause_effect_corr &lt;- monthly_cause_effect[, c(\"NUM_BAITS\", \"Lagged_Sightings\")]\n\n# Compute correlation matrix\nmonthly_correlation_matrix &lt;- cor(monthly_cause_effect_corr,\n                                  use = \"complete.obs\")\n\n# Generate correlation plot\nsuppressWarnings(correlation_plot &lt;- corrplot(\n  monthly_correlation_matrix, \n  method = \"circle\", \n  type = \"full\", \n  col = colorRampPalette(c(\"plum\", \"white\", \"purple\"))(200), \n  addCoef.col = \"black\", \n  number.cex = 0.7, \n  tl.col = \"black\", \n  tl.cex = 0.8, \n  cl.cex = 0.8, \n  title = \"Figure 9: Correlation of Monthly Bait Inspections and Next-Month Sightings\", \n  mar = c(0, 0, 2, 0)))\n\n\n\n\n\n\n\n\n\nBased on the correlation coefficient of 0.21, we can determine that, in general, as the number of bait inspections increases in one month, the number of rat sightings tends to increase in the following year, but with very weak strength. There is almost no correlation. This means that on a monthly basis, bait inspections have no effect on rat sightings. It can be assumed that examining this from a monthly lens is too short of a time frame to see effective results."
  },
  {
    "objectID": "individual_report.html#comparing-sightings-to-bait-inspections---borough",
    "href": "individual_report.html#comparing-sightings-to-bait-inspections---borough",
    "title": "NYC DOH Programs and the Rat Population in NYC",
    "section": "Comparing Sightings to Bait Inspections - Borough",
    "text": "Comparing Sightings to Bait Inspections - Borough\nAfter examining from a time lens, we’ll examine per borough per year.\n\n\nCode\n# Group by BOROUGH and YEAR and calculate the count of sightings\nsightings_per_borough_per_year &lt;- rat_sightings |&gt;\n  mutate(YEAR = as.numeric(format(as.Date(CREATED_DATE, \"%Y-%m-%d\"), \n                                  \"%Y\"))) |&gt;  # Extract year\n  filter(YEAR &gt;= 2019 & YEAR &lt;= 2024) |&gt;\n  group_by(YEAR, BOROUGH) |&gt;  # Group by year and borough\n  summarize(SIGHTINGS = n(), .groups = 'drop')  # Count sightings\n\n# Combine with bait_per_year\nreport_borough_year &lt;- sightings_per_borough_per_year %&gt;%\n  left_join(bait_per_year, \n            by = c(\"YEAR\" = \"Year\")) |&gt; # Join on the 'YEAR' column\n  rename(Bait = Count) \n\n\n\nBronx\nFirst we’ll look at the Bronx.\n\n\nCode\n# Filter sightings data for BRONX and lag it by 1 year\nsightings_lagged_bronx &lt;- sightings_per_borough_per_year |&gt;\n  filter(BOROUGH == \"BRONX\") |&gt;  # Focus on BRONX\n  mutate(YEAR = YEAR - 1) |&gt;  # Lag the year by 1\n  rename(Lagged_Sightings = SIGHTINGS)\n\n# Merge lagged sightings with bait data on YEAR\nyearly_cause_effect_bronx &lt;- bait_per_year |&gt;\n  rename(Bait = Count) |&gt;  # Rename Count to Bait\n  inner_join(sightings_lagged_bronx, by = c(\"Year\" = \"YEAR\"))\n\n# Create a correlation matrix for Bait and Lagged Sightings\nyearly_cause_effect_corr_bronx &lt;- yearly_cause_effect_bronx[, c(\"Bait\", \"Lagged_Sightings\")]\n\n# Compute correlation matrix\nyearly_correlation_matrix_bronx &lt;- cor(yearly_cause_effect_corr_bronx, \n                                       use = \"complete.obs\")\n\n# Generate correlation plot\nsuppressWarnings(correlation_plot_bronx &lt;- corrplot(\n  yearly_correlation_matrix_bronx,\n  method = \"circle\",\n  type = \"full\",\n  col = colorRampPalette(c(\"plum\", \"white\", \"purple\"))(200),\n  addCoef.col = \"black\",\n  number.cex = 0.7,\n  tl.col = \"black\",\n  tl.cex = 0.8,\n  cl.cex = 0.8,\n  title = \"Figure 10: Correlation of Bait and Next-Year Sightings (Bronx)\",\n  mar = c(0, 0, 2, 0)))\n\n\n\n\n\n\n\n\n\nBased on the correlation coefficient of -0.84, we can determine that there is a strong correlation between the number of bait inspections increasing in one year and the number of rat sightings decreasing the following year in the Bronx.\nThis means that in the Bronx, higher bait inspections has a beneficial impact on reducing sightings on a yearly basis.\n\n\nBrooklyn\nSecond, we’ll look at Brooklyn.\n\n\nCode\n# Filter sightings data for BROOKLYN and lag it by 1 year\nsightings_lagged_brooklyn &lt;- sightings_per_borough_per_year |&gt;\n  filter(BOROUGH == \"BROOKLYN\") |&gt;\n  mutate(YEAR = YEAR - 1) |&gt;\n  rename(Lagged_Sightings = SIGHTINGS)\n\n# Merge lagged sightings with bait data on YEAR\nyearly_cause_effect_brooklyn &lt;- bait_per_year |&gt;\n  rename(Bait = Count) |&gt;\n  inner_join(sightings_lagged_brooklyn, by = c(\"Year\" = \"YEAR\"))\n\n# Create a correlation matrix for Bait and Lagged Sightings\nyearly_cause_effect_corr_brooklyn &lt;- yearly_cause_effect_brooklyn[, c(\"Bait\", \"Lagged_Sightings\")]\n\n# Compute correlation matrix\nyearly_correlation_matrix_brooklyn &lt;- cor(yearly_cause_effect_corr_brooklyn, \n                                          use = \"complete.obs\")\n\n# Generate correlation plot\nsuppressWarnings(correlation_plot_brooklyn &lt;- corrplot(\n  yearly_correlation_matrix_brooklyn,\n  method = \"circle\",\n  type = \"full\",\n  col = colorRampPalette(c(\"plum\", \"white\", \"purple\"))(200),\n  addCoef.col = \"black\",\n  number.cex = 0.7,\n  tl.col = \"black\",\n  tl.cex = 0.8,\n  cl.cex = 0.8,\n  title = \"Figure 11: Correlation of Bait and Next-Year Sightings (Brooklyn)\",\n  mar = c(0, 0, 2, 0)\n))\n\n\n\n\n\n\n\n\n\nBased on the correlation coefficient of -0.39, we can determine that there is a weak correlation between the number of bait inspections increasing in one year and the number of rat sightings decreasing the following year in Brooklyn.\nThis means that in Brooklyn, higher bait inspections has a somewhat weak, beneficial impact on reducing sightings on a yearly basis.\n\n\nManhattan\nThird, we’ll look at Manhattan.\n\n\nCode\n# Filter sightings data for MANHATTAN and lag it by 1 year\nsightings_lagged_manhattan &lt;- sightings_per_borough_per_year |&gt;\n  filter(BOROUGH == \"MANHATTAN\") |&gt;\n  mutate(YEAR = YEAR - 1) |&gt;\n  rename(Lagged_Sightings = SIGHTINGS)\n\n# Merge lagged sightings with bait data on YEAR\nyearly_cause_effect_manhattan &lt;- bait_per_year |&gt;\n  rename(Bait = Count) |&gt;\n  inner_join(sightings_lagged_manhattan, by = c(\"Year\" = \"YEAR\"))\n\n# Create a correlation matrix for Bait and Lagged Sightings\nyearly_cause_effect_corr_manhattan &lt;- yearly_cause_effect_manhattan[, c(\"Bait\", \"Lagged_Sightings\")]\n\n# Compute correlation matrix\nyearly_correlation_matrix_manhattan &lt;- cor(yearly_cause_effect_corr_manhattan, \n                                           use = \"complete.obs\")\n\n# Generate correlation plot\nsuppressWarnings(correlation_plot_manhattan &lt;- corrplot(\n  yearly_correlation_matrix_manhattan,\n  method = \"circle\",\n  type = \"full\",\n  col = colorRampPalette(c(\"plum\", \"white\", \"purple\"))(200),\n  addCoef.col = \"black\",\n  number.cex = 0.7,\n  tl.col = \"black\",\n  tl.cex = 0.8,\n  cl.cex = 0.8,\n  title = \"Figure 12: Correlation of Bait and Next-Year Sightings (Manhattan)\",\n  mar = c(0, 0, 2, 0)\n))\n\n\n\n\n\n\n\n\n\nBased on the correlation coefficient of -0.78, we can determine that there is a strong correlation between the number of bait inspections increasing in one year and the number of rat sightings decreasing the following year in Manhattan.\nThis means that in Manhattan, higher bait inspections has a beneficial impact on reducing sightings on a yearly basis.\n\n\nQueens\nNext, we’ll look at Queens.\n\n\nCode\n# Filter sightings data for QUEENS and lag it by 1 year\nsightings_lagged_queens &lt;- sightings_per_borough_per_year |&gt;\n  filter(BOROUGH == \"QUEENS\") |&gt;\n  mutate(YEAR = YEAR - 1) |&gt;\n  rename(Lagged_Sightings = SIGHTINGS)\n\n# Merge lagged sightings with bait data on YEAR\nyearly_cause_effect_queens &lt;- bait_per_year |&gt;\n  rename(Bait = Count) |&gt;\n  inner_join(sightings_lagged_queens, by = c(\"Year\" = \"YEAR\"))\n\n# Create a correlation matrix for Bait and Lagged Sightings\nyearly_cause_effect_corr_queens &lt;- yearly_cause_effect_queens[, \n                                                              c(\"Bait\", \"Lagged_Sightings\")]\n\n# Compute correlation matrix\nyearly_correlation_matrix_queens &lt;- cor(yearly_cause_effect_corr_queens, \n                                        use = \"complete.obs\")\n\n# Generate correlation plot\nsuppressWarnings(correlation_plot_queens &lt;- corrplot(\n  yearly_correlation_matrix_queens,\n  method = \"circle\",\n  type = \"full\",\n  col = colorRampPalette(c(\"plum\", \"white\", \"purple\"))(200),\n  addCoef.col = \"black\",\n  number.cex = 0.7,\n  tl.col = \"black\",\n  tl.cex = 0.8,\n  cl.cex = 0.8,\n  title = \"Figure 13: Correlation of Bait and Next-Year Sightings (Queens)\",\n  mar = c(0, 0, 2, 0)\n))\n\n\n\n\n\n\n\n\n\nBased on the correlation coefficient of 0.13, we can determine that, in general in Queens, as the number of bait inspections increases in one year, the number of rat sightings tends to increase in the following year, but with very weak strength. There is almost no correlation.\nThis means that in Queens, on a yearly basis, bait inspections have no effect on rat sightings.\n\n\nStaten Island\nFinally, we’ll look at Staten Island.\n\n\nCode\n# Filter sightings data for STATEN ISLAND and lag it by 1 year\nsightings_lagged_staten_island &lt;- sightings_per_borough_per_year |&gt;\n  filter(BOROUGH == \"STATEN ISLAND\") |&gt;\n  mutate(YEAR = YEAR - 1) |&gt;\n  rename(Lagged_Sightings = SIGHTINGS)\n\n# Merge lagged sightings with bait data on YEAR\nyearly_cause_effect_staten_island &lt;- bait_per_year |&gt;\n  rename(Bait = Count) |&gt;\n  inner_join(sightings_lagged_staten_island, by = c(\"Year\" = \"YEAR\"))\n\n# Create a correlation matrix for Bait and Lagged Sightings\nyearly_cause_effect_corr_staten_island &lt;- yearly_cause_effect_staten_island[, c(\"Bait\", \"Lagged_Sightings\")]\n\n# Compute correlation matrix\nyearly_correlation_matrix_staten_island &lt;- cor(yearly_cause_effect_corr_staten_island, \n                                               use = \"complete.obs\")\n\n# Generate correlation plot\nsuppressWarnings(correlation_plot_staten_island &lt;- corrplot(\n  yearly_correlation_matrix_staten_island,\n  method = \"circle\",\n  type = \"full\",\n  col = colorRampPalette(c(\"plum\", \"white\", \"purple\"))(200),\n  addCoef.col = \"black\",\n  number.cex = 0.7,\n  tl.col = \"black\",\n  tl.cex = 0.8,\n  cl.cex = 0.8,\n  title = \"Figure 14: Correlation of Bait and Next-Year Sightings (Staten Island)\",\n  mar = c(0, 0, 2, 0)\n))\n\n\n\n\n\n\n\n\n\nBased on the correlation coefficient of -0.76, we can determine that there is a strong correlation between the number of bait inspections increasing in one year and the number of rat sightings decreasing the following year in Staten Island.\nThis means that in Staten Island, higher bait inspections has a beneficial impact on reducing sightings on a yearly basis."
  },
  {
    "objectID": "group_report.html",
    "href": "group_report.html",
    "title": "Rat Pack - Final Group Report",
    "section": "",
    "text": "Authors: Thanh Dao, Valeriia Frolova, Aachal Grimire, Elijah Yong, Gracie Zheng\nLast Updated: December 19th, 2024 @1:25PM"
  },
  {
    "objectID": "group_report.html#specific-topics",
    "href": "group_report.html#specific-topics",
    "title": "Rat Pack - Final Group Report",
    "section": "Specific Topics",
    "text": "Specific Topics\nTo better answer our overarching question, we focused on the following topics:\n\nThe effect of NYC DOH programs (report)\nThe effect of annual income and unemployment of a neighborhood (report)\nThe effect of restaurants (report)\nThe effect of food scrap drop-off sites (report)\nThe effect of NYC public transportation (report)\n\nThe corresponding, in-depth reports are linked with each topic."
  },
  {
    "objectID": "group_report.html#limitations",
    "href": "group_report.html#limitations",
    "title": "Rat Pack - Final Group Report",
    "section": "Limitations",
    "text": "Limitations\nThe data used in our analysis, though sourced from government agencies, has notable limitations. One key issue involves reporting and location-based biases in data sets like rat sightings and inspections. These rely on reported incidents, meaning actual rat activity may be underrepresented or over-represented depending on residents’ willingness or ability to report sightings. This leads to an incomplete picture of infestations across the city.\nAdditionally, some data sets lacked context, with non-descriptive or missing values in certain columns. While cross-referencing with other sources helped mitigate this, it posed challenges to achieving a thorough understanding of underlying patterns.\nA significant gap was the absence of MTA data, critical for exploring the link between public transportation and rat activity. This forced reliance on extrapolations and assumptions, which may not fully capture transit-related nuances.\nDespite these constraints, the data provided valuable insights into broad trends, forming a strong foundation for analysis."
  },
  {
    "objectID": "group_report.html#nyc-doh",
    "href": "group_report.html#nyc-doh",
    "title": "Rat Pack - Final Group Report",
    "section": "NYC DOH",
    "text": "NYC DOH\nWe analyzed the impact of NYC DOH bait programs on rat sightings using data from OpenData, specifically focusing on rat sightings and rat inspections. The analysis uses a cause-and-effect approach, comparing the number of bait inspections in one year to rat sightings in the following year, and drawing conclusions based on a calculated correlation coefficient.\nOverall, on a yearly basis, the data reveals a moderately strong correlation, suggesting that increased bait inspections are somewhat effective in reducing rat sightings. However, when examined on a monthly basis, no significant relationship was observed. This indicates that a monthly time frame may be too short to capture measurable effects.\nFocusing instead on a yearly perspective across boroughs, the findings reveal that bait programs are highly effective in the Bronx, showing a strong correlation with reduced sightings. In contrast, no correlation was found in Queens, suggesting the programs may have little to no impact there.\nThe following are 2 sample figures created throughout this process, and display the correlation between bait inspections and next year’s sightings. To the left is the overall, while to the right is specifically for the Bronx. Further figures and analysis can be found in the NYC DOH report linked above.\n\n\n\nFigures for DOH Report\n\n\nOverall, we can say that NYC DOH programs, specifically their bait initiative, are beneficial in decreasing the rat sightings in NYC on a yearly basis."
  },
  {
    "objectID": "group_report.html#income-and-unemployment",
    "href": "group_report.html#income-and-unemployment",
    "title": "Rat Pack - Final Group Report",
    "section": "Income and Unemployment",
    "text": "Income and Unemployment\nWe looked into the impact of socioeconomic factors on rat populations, more specifically how annual median income and unemployment rates influence rat sightings. To get a sense of this on a borough level, we created two plots that compared our variables to the number of rat sightings which both show little to no correlation between the two.\n\n\n\nFigures for Income and Unemployment per Borough\n\n\nThe issue we originally faced is that it is hard to draw conclusions from just five data points and unemployment rates/annual median income are not equally distributed throughout boroughs. To remedy this, we broke down the city into community districts which showed the variation in income and unemployment per borough.\n\n\n\nFigures for Borough Variation\n\n\nAs we can see on the left, there are certain outliers like Williamsburg and the upper west side where the median income is vastly greater than other respective parts of Brooklyn and Manhattan. On the right, we can see that in terms of unemployment, there is a heavy concentration in the upper regions of Manhattan and across the Bronx.\n\n\n\nFigures for Income and Unemployment cdta\n\n\nAfter taking a more granular look into communities and adding more data points, we still could not find any correlation between median income, unemployment rates and number of rat sightings. While removing outliers and running a correlation analysis, the correlation values are a 0.0085 between median income and rat sightings and 0.15 correlation between unemployment and rat sightings. Although there may be some minor relationship between unemployment and rat sightings, these findings suggest that other factors may play a more significant role in determining rat populations."
  },
  {
    "objectID": "group_report.html#restaurants",
    "href": "group_report.html#restaurants",
    "title": "Rat Pack - Final Group Report",
    "section": "Restaurants",
    "text": "Restaurants\nNew York City’s ongoing battle with rat infestations is a multifaceted issue, with poor restaurant sanitation playing a significant role. By analyzing restaurant inspection grades, violation data, and rat activity across various boroughs, this report uncovers the connection between poorly rated restaurants (Grades B and C) and rat infestations. Additionally, we examine the impact of outdoor dining trends and restaurant density on rat activity, providing borough-specific insights to highlight areas of concern and potential solutions.\n\nPoor Restaurant Sanitation\n\n\n\nRestaurants with Lower Grades in Different Zip Codes\n\n\nBrooklyn’s ZIP codes 11215 and 11222 have the highest rat densities (over 20,000), while the Bronx 10457 and 10458 show high densities (over 12,000), both linked to poor sanitation. Queens’ 11385 and 11373 also see peaks, while Manhattan’s 10013 and 10036 show moderate rat activity despite many poorly graded restaurants. Staten Island 10301 reports the lowest rat activity with minimal infestations.\n\n\nRestaurant Density per Neighborhood\nOur analysis reveals a positive correlation between restaurant density and rat infestations. Manhattan (~94,661 restaurants) and Brooklyn (~69,797) showed the highest rat activity, with 136,963 and 145,694 inspections, respectively, driven by increased food waste. The Bronx, an outlier, reported high rat activity (~96,498 inspections) despite moderate restaurant density (~23,274), indicating other factors like waste management may contribute. Conversely, Staten Island, with fewer restaurants (~9,218), reported minimal activity (~2,049 inspections), supporting the link between restaurant density and infestations.\n ### Outdoor Dining\nThe rise of outdoor dining in NYC during the COVID-19 pandemic brought new challenges in controlling rat populations. Comparing 2019 pre-pandemic data to post-COVID years, we observed a sharp increase in outdoor seating violations and rat infestations, peaking in 2023 and 2024 as outdoor dining expanded.\nThe article “The new Normal, Ah!! Rats!! by Elazar Sontag (Eater, December 14, 2021) supports our findings, explaining how restaurant reopenings and outdoor dining created food waste that fueled rat resurgence. Initially, rat populations declined during restaurant closures, but cities like Chicago, Los Angeles, and New Orleans faced similar challenges, showing this is a broader urban issue.\n\n\n\nFigure on Outdoor Seating and Rat Infestations\n\n\nOur analysis highlights a strong connection between restaurant density, poor sanitation, outdoor dining, and rising rat activity in NYC, especially post-COVID. Boroughs like Brooklyn and the Bronx face significant infestations due to dense, poorly graded restaurants and improper waste management."
  },
  {
    "objectID": "group_report.html#food-drop-off-sites",
    "href": "group_report.html#food-drop-off-sites",
    "title": "Rat Pack - Final Group Report",
    "section": "Food Drop-Off Sites",
    "text": "Food Drop-Off Sites\nThe findings of this analysis provide valuable insights into the relationship between food scrap drop-off sites and rat complaints in NYC, but they do not conclusively indicate that these sites are the direct cause of infestations.\nOver the past five years, we observed a total of 98,141 rat complaints in proximity to food scrap locations, with an average of 773 complaints per ZIP code. A moderate correlation of 0.59 between the number of bins and complaints suggests a potential link, but this alone does not establish causation.\nSeasonal trends reveal higher complaints during summer and early fall (June to October), likely driven by increased rodent activity in warmer months, while complaints drop significantly during the winter.\n\n\n\nFigures for Top Monthly Complaints Zip Codes\n\n\nYear-to-year fluctuations show peaks in specific years, particularly in areas with frequent complaints, reflecting broader patterns that may be influenced by external factors.\n\n\n\nFigures for Top 3 Most Complained Zips by Years\n\n\nImportantly, the analysis highlights that the presence of bins does not always equate to higher infestations. ZIP codes with only one food scrap bin, such as those with a high complaints-to-bin ratio, demonstrate that factors beyond infrastructure—such as population density, socioeconomic conditions, and improper usage—also play a role.\n\n\n\nFigures for Top Complaints Per Site\n\n\nProper disposal, securely closed bins, and community compliance are key to this program’s success. With bins expanding citywide and mandatory composting rules and fines beginning in spring 2025, resident adherence is essential to overcoming current challenges.\nFurther research is needed to examine factors influencing rodent activity near food scrap bins, including socioeconomic conditions, pedestrian traffic, and bin design (e.g., metal vs. plastic). As the program grows, monitoring the impact of new rules and public compliance on rat complaints will offer valuable insights. These findings emphasize the need for a multifaceted strategy to tackle urban sustainability and public health challenges."
  },
  {
    "objectID": "group_report.html#public-transportation",
    "href": "group_report.html#public-transportation",
    "title": "Rat Pack - Final Group Report",
    "section": "Public Transportation",
    "text": "Public Transportation\nWe analyzed the potential relationship between public transportation and rat sightings, focusing on the MTA’s subway and bus systems. Examining the overlap between rat sightings and transit locations across NYC, we found a weak correlation between where rats are present and where people travel.\nOur analysis explored rat sightings near subway stations and bus stops. Overall, sightings were distributed across the city, with no strong alignment to transit hubs. However, specific areas, such as parts of Staten Island and the Rockaways, showed some overlap with transit lines, suggesting localized patterns that warrant further investigation.\n\n\n\nFigures for Rat Sightings vs Transportation Modes\n\n\n\n\n\nCorrelation between Rat Sightings and Transportation Modes\n\n\nThrough correlation analysis, location points are quantified by calculating the difference between the sighting and the station/subway. We can see that the further away the location of transportation is, the less sightings. However, there is actually a weak negative relationship for both, -.25 on subways and -.41 on buses. Therefore, rat sightings cannot be explained strongly by public transportation locations.\nThe second investigation looked at the number of public transportation per neighborhood compared to rat sightings per neighborhood.\n\n\n\nFigures for NYC Maps\n\n\nThese graphs show an initial analysis of the count of each throughout NYC’s neighborhood. When we glance over these graphs, we see that sightings are more concentrated in the West side of Manhattan and the upper half of Brooklyn, where the train stations seem to also be slightly concentrated.\n\n\n\nFigures for Correlation Analysis\n\n\nWhen we do our correlation analysis, as the number of stations increase, there is a slight increase in rat sightings. However, there is weak correlation, at .37 subways and .25 stops.\nThough we see that there is a weak correlation between rat sightings and public transportation, as you read into the individual report, you may see how difficult it was to gather such data points. The data is not perfect so, many points were cleaned away but, if sighting data was wholly collected by the MTA agency and location points reported perfectly, the answer to this question may change."
  }
]